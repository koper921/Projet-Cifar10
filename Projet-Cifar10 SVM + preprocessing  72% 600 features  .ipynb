{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "#from scipy.misc import imread\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import image\n",
    "\n",
    "from collections import Counter\n",
    "from scipy.stats.mstats import mode\n",
    "\n",
    "def load_CIFAR_batch(filename):\n",
    "  \"\"\" load single batch of cifar \"\"\"\n",
    "  with open(filename, 'rb') as f:\n",
    "    datadict = pickle.load(f,encoding='latin1')\n",
    "    X = datadict['data']\n",
    "    \n",
    "    Y = datadict['labels']\n",
    "    X = X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"double\")\n",
    "    Y = np.array(Y)\n",
    "    return X, Y\n",
    "\n",
    "def load_CIFAR10(ROOT):\n",
    "  \"\"\" load all of cifar \"\"\"\n",
    "  xs = []\n",
    "  ys = []\n",
    "  for b in range(1,6):\n",
    "    f = os.path.join(ROOT, 'data_batch_%d' % (b, ))\n",
    "    X, Y = load_CIFAR_batch(f)\n",
    "    xs.append(X)\n",
    "    ys.append(Y)    \n",
    "  Xtr = np.concatenate(xs)\n",
    "  Ytr = np.concatenate(ys)\n",
    "  del X, Y\n",
    "  Xte, Yte = load_CIFAR_batch(os.path.join(ROOT, 'test_batch'))\n",
    "#Normalise la base - la moyenne , divise par std, resultat final toujours mauvais normalisé ou non\n",
    "#Normalize the data, by soustracting the mean and dividing the standart deviation \n",
    "  mean_image = np.mean(Xtr, axis=0)\n",
    "  std_image=np.std(Xtr, axis=0)\n",
    "  #Xtr -= mean_image\n",
    "  #Xtr /= std_image\n",
    "  \n",
    "  #Xte -= mean_image\n",
    "  #Xte /= std_image\n",
    "  return Xtr, Ytr, Xte, Yte\n",
    "\n",
    "\n",
    "\n",
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000):\n",
    "    cifar10_dir = 'C:/Users/MyPC/Desktop/Projet Apprentissage Image/cifar-10-batches-py/'\n",
    "    \n",
    "    print (len(load_CIFAR10(cifar10_dir)))\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mean_image = np.mean(X_train, axis=0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_train=X_train.swapaxes(1,3)\n",
    "    X_val=X_val.swapaxes(1,3)\n",
    "    X_test=X_test.swapaxes(1,3)\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test=get_CIFAR10_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction patche: 0 / 400000\n",
      "Extraction patche: 1000 / 400000\n",
      "Extraction patche: 2000 / 400000\n",
      "Extraction patche: 3000 / 400000\n",
      "Extraction patche: 4000 / 400000\n",
      "Extraction patche: 5000 / 400000\n",
      "Extraction patche: 6000 / 400000\n",
      "Extraction patche: 7000 / 400000\n",
      "Extraction patche: 8000 / 400000\n",
      "Extraction patche: 9000 / 400000\n",
      "Extraction patche: 10000 / 400000\n",
      "Extraction patche: 11000 / 400000\n",
      "Extraction patche: 12000 / 400000\n",
      "Extraction patche: 13000 / 400000\n",
      "Extraction patche: 14000 / 400000\n",
      "Extraction patche: 15000 / 400000\n",
      "Extraction patche: 16000 / 400000\n",
      "Extraction patche: 17000 / 400000\n",
      "Extraction patche: 18000 / 400000\n",
      "Extraction patche: 19000 / 400000\n",
      "Extraction patche: 20000 / 400000\n",
      "Extraction patche: 21000 / 400000\n",
      "Extraction patche: 22000 / 400000\n",
      "Extraction patche: 23000 / 400000\n",
      "Extraction patche: 24000 / 400000\n",
      "Extraction patche: 25000 / 400000\n",
      "Extraction patche: 26000 / 400000\n",
      "Extraction patche: 27000 / 400000\n",
      "Extraction patche: 28000 / 400000\n",
      "Extraction patche: 29000 / 400000\n",
      "Extraction patche: 30000 / 400000\n",
      "Extraction patche: 31000 / 400000\n",
      "Extraction patche: 32000 / 400000\n",
      "Extraction patche: 33000 / 400000\n",
      "Extraction patche: 34000 / 400000\n",
      "Extraction patche: 35000 / 400000\n",
      "Extraction patche: 36000 / 400000\n",
      "Extraction patche: 37000 / 400000\n",
      "Extraction patche: 38000 / 400000\n",
      "Extraction patche: 39000 / 400000\n",
      "Extraction patche: 40000 / 400000\n",
      "Extraction patche: 41000 / 400000\n",
      "Extraction patche: 42000 / 400000\n",
      "Extraction patche: 43000 / 400000\n",
      "Extraction patche: 44000 / 400000\n",
      "Extraction patche: 45000 / 400000\n",
      "Extraction patche: 46000 / 400000\n",
      "Extraction patche: 47000 / 400000\n",
      "Extraction patche: 48000 / 400000\n",
      "Extraction patche: 49000 / 400000\n",
      "Extraction patche: 50000 / 400000\n",
      "Extraction patche: 51000 / 400000\n",
      "Extraction patche: 52000 / 400000\n",
      "Extraction patche: 53000 / 400000\n",
      "Extraction patche: 54000 / 400000\n",
      "Extraction patche: 55000 / 400000\n",
      "Extraction patche: 56000 / 400000\n",
      "Extraction patche: 57000 / 400000\n",
      "Extraction patche: 58000 / 400000\n",
      "Extraction patche: 59000 / 400000\n",
      "Extraction patche: 60000 / 400000\n",
      "Extraction patche: 61000 / 400000\n",
      "Extraction patche: 62000 / 400000\n",
      "Extraction patche: 63000 / 400000\n",
      "Extraction patche: 64000 / 400000\n",
      "Extraction patche: 65000 / 400000\n",
      "Extraction patche: 66000 / 400000\n",
      "Extraction patche: 67000 / 400000\n",
      "Extraction patche: 68000 / 400000\n",
      "Extraction patche: 69000 / 400000\n",
      "Extraction patche: 70000 / 400000\n",
      "Extraction patche: 71000 / 400000\n",
      "Extraction patche: 72000 / 400000\n",
      "Extraction patche: 73000 / 400000\n",
      "Extraction patche: 74000 / 400000\n",
      "Extraction patche: 75000 / 400000\n",
      "Extraction patche: 76000 / 400000\n",
      "Extraction patche: 77000 / 400000\n",
      "Extraction patche: 78000 / 400000\n",
      "Extraction patche: 79000 / 400000\n",
      "Extraction patche: 80000 / 400000\n",
      "Extraction patche: 81000 / 400000\n",
      "Extraction patche: 82000 / 400000\n",
      "Extraction patche: 83000 / 400000\n",
      "Extraction patche: 84000 / 400000\n",
      "Extraction patche: 85000 / 400000\n",
      "Extraction patche: 86000 / 400000\n",
      "Extraction patche: 87000 / 400000\n",
      "Extraction patche: 88000 / 400000\n",
      "Extraction patche: 89000 / 400000\n",
      "Extraction patche: 90000 / 400000\n",
      "Extraction patche: 91000 / 400000\n",
      "Extraction patche: 92000 / 400000\n",
      "Extraction patche: 93000 / 400000\n",
      "Extraction patche: 94000 / 400000\n",
      "Extraction patche: 95000 / 400000\n",
      "Extraction patche: 96000 / 400000\n",
      "Extraction patche: 97000 / 400000\n",
      "Extraction patche: 98000 / 400000\n",
      "Extraction patche: 99000 / 400000\n",
      "Extraction patche: 100000 / 400000\n",
      "Extraction patche: 101000 / 400000\n",
      "Extraction patche: 102000 / 400000\n",
      "Extraction patche: 103000 / 400000\n",
      "Extraction patche: 104000 / 400000\n",
      "Extraction patche: 105000 / 400000\n",
      "Extraction patche: 106000 / 400000\n",
      "Extraction patche: 107000 / 400000\n",
      "Extraction patche: 108000 / 400000\n",
      "Extraction patche: 109000 / 400000\n",
      "Extraction patche: 110000 / 400000\n",
      "Extraction patche: 111000 / 400000\n",
      "Extraction patche: 112000 / 400000\n",
      "Extraction patche: 113000 / 400000\n",
      "Extraction patche: 114000 / 400000\n",
      "Extraction patche: 115000 / 400000\n",
      "Extraction patche: 116000 / 400000\n",
      "Extraction patche: 117000 / 400000\n",
      "Extraction patche: 118000 / 400000\n",
      "Extraction patche: 119000 / 400000\n",
      "Extraction patche: 120000 / 400000\n",
      "Extraction patche: 121000 / 400000\n",
      "Extraction patche: 122000 / 400000\n",
      "Extraction patche: 123000 / 400000\n",
      "Extraction patche: 124000 / 400000\n",
      "Extraction patche: 125000 / 400000\n",
      "Extraction patche: 126000 / 400000\n",
      "Extraction patche: 127000 / 400000\n",
      "Extraction patche: 128000 / 400000\n",
      "Extraction patche: 129000 / 400000\n",
      "Extraction patche: 130000 / 400000\n",
      "Extraction patche: 131000 / 400000\n",
      "Extraction patche: 132000 / 400000\n",
      "Extraction patche: 133000 / 400000\n",
      "Extraction patche: 134000 / 400000\n",
      "Extraction patche: 135000 / 400000\n",
      "Extraction patche: 136000 / 400000\n",
      "Extraction patche: 137000 / 400000\n",
      "Extraction patche: 138000 / 400000\n",
      "Extraction patche: 139000 / 400000\n",
      "Extraction patche: 140000 / 400000\n",
      "Extraction patche: 141000 / 400000\n",
      "Extraction patche: 142000 / 400000\n",
      "Extraction patche: 143000 / 400000\n",
      "Extraction patche: 144000 / 400000\n",
      "Extraction patche: 145000 / 400000\n",
      "Extraction patche: 146000 / 400000\n",
      "Extraction patche: 147000 / 400000\n",
      "Extraction patche: 148000 / 400000\n",
      "Extraction patche: 149000 / 400000\n",
      "Extraction patche: 150000 / 400000\n",
      "Extraction patche: 151000 / 400000\n",
      "Extraction patche: 152000 / 400000\n",
      "Extraction patche: 153000 / 400000\n",
      "Extraction patche: 154000 / 400000\n",
      "Extraction patche: 155000 / 400000\n",
      "Extraction patche: 156000 / 400000\n",
      "Extraction patche: 157000 / 400000\n",
      "Extraction patche: 158000 / 400000\n",
      "Extraction patche: 159000 / 400000\n",
      "Extraction patche: 160000 / 400000\n",
      "Extraction patche: 161000 / 400000\n",
      "Extraction patche: 162000 / 400000\n",
      "Extraction patche: 163000 / 400000\n",
      "Extraction patche: 164000 / 400000\n",
      "Extraction patche: 165000 / 400000\n",
      "Extraction patche: 166000 / 400000\n",
      "Extraction patche: 167000 / 400000\n",
      "Extraction patche: 168000 / 400000\n",
      "Extraction patche: 169000 / 400000\n",
      "Extraction patche: 170000 / 400000\n",
      "Extraction patche: 171000 / 400000\n",
      "Extraction patche: 172000 / 400000\n",
      "Extraction patche: 173000 / 400000\n",
      "Extraction patche: 174000 / 400000\n",
      "Extraction patche: 175000 / 400000\n",
      "Extraction patche: 176000 / 400000\n",
      "Extraction patche: 177000 / 400000\n",
      "Extraction patche: 178000 / 400000\n",
      "Extraction patche: 179000 / 400000\n",
      "Extraction patche: 180000 / 400000\n",
      "Extraction patche: 181000 / 400000\n",
      "Extraction patche: 182000 / 400000\n",
      "Extraction patche: 183000 / 400000\n",
      "Extraction patche: 184000 / 400000\n",
      "Extraction patche: 185000 / 400000\n",
      "Extraction patche: 186000 / 400000\n",
      "Extraction patche: 187000 / 400000\n",
      "Extraction patche: 188000 / 400000\n",
      "Extraction patche: 189000 / 400000\n",
      "Extraction patche: 190000 / 400000\n",
      "Extraction patche: 191000 / 400000\n",
      "Extraction patche: 192000 / 400000\n",
      "Extraction patche: 193000 / 400000\n",
      "Extraction patche: 194000 / 400000\n",
      "Extraction patche: 195000 / 400000\n",
      "Extraction patche: 196000 / 400000\n",
      "Extraction patche: 197000 / 400000\n",
      "Extraction patche: 198000 / 400000\n",
      "Extraction patche: 199000 / 400000\n",
      "Extraction patche: 200000 / 400000\n",
      "Extraction patche: 201000 / 400000\n",
      "Extraction patche: 202000 / 400000\n",
      "Extraction patche: 203000 / 400000\n",
      "Extraction patche: 204000 / 400000\n",
      "Extraction patche: 205000 / 400000\n",
      "Extraction patche: 206000 / 400000\n",
      "Extraction patche: 207000 / 400000\n",
      "Extraction patche: 208000 / 400000\n",
      "Extraction patche: 209000 / 400000\n",
      "Extraction patche: 210000 / 400000\n",
      "Extraction patche: 211000 / 400000\n",
      "Extraction patche: 212000 / 400000\n",
      "Extraction patche: 213000 / 400000\n",
      "Extraction patche: 214000 / 400000\n",
      "Extraction patche: 215000 / 400000\n",
      "Extraction patche: 216000 / 400000\n",
      "Extraction patche: 217000 / 400000\n",
      "Extraction patche: 218000 / 400000\n",
      "Extraction patche: 219000 / 400000\n",
      "Extraction patche: 220000 / 400000\n",
      "Extraction patche: 221000 / 400000\n",
      "Extraction patche: 222000 / 400000\n",
      "Extraction patche: 223000 / 400000\n",
      "Extraction patche: 224000 / 400000\n",
      "Extraction patche: 225000 / 400000\n",
      "Extraction patche: 226000 / 400000\n",
      "Extraction patche: 227000 / 400000\n",
      "Extraction patche: 228000 / 400000\n",
      "Extraction patche: 229000 / 400000\n",
      "Extraction patche: 230000 / 400000\n",
      "Extraction patche: 231000 / 400000\n",
      "Extraction patche: 232000 / 400000\n",
      "Extraction patche: 233000 / 400000\n",
      "Extraction patche: 234000 / 400000\n",
      "Extraction patche: 235000 / 400000\n",
      "Extraction patche: 236000 / 400000\n",
      "Extraction patche: 237000 / 400000\n",
      "Extraction patche: 238000 / 400000\n",
      "Extraction patche: 239000 / 400000\n",
      "Extraction patche: 240000 / 400000\n",
      "Extraction patche: 241000 / 400000\n",
      "Extraction patche: 242000 / 400000\n",
      "Extraction patche: 243000 / 400000\n",
      "Extraction patche: 244000 / 400000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction patche: 245000 / 400000\n",
      "Extraction patche: 246000 / 400000\n",
      "Extraction patche: 247000 / 400000\n",
      "Extraction patche: 248000 / 400000\n",
      "Extraction patche: 249000 / 400000\n",
      "Extraction patche: 250000 / 400000\n",
      "Extraction patche: 251000 / 400000\n",
      "Extraction patche: 252000 / 400000\n",
      "Extraction patche: 253000 / 400000\n",
      "Extraction patche: 254000 / 400000\n",
      "Extraction patche: 255000 / 400000\n",
      "Extraction patche: 256000 / 400000\n",
      "Extraction patche: 257000 / 400000\n",
      "Extraction patche: 258000 / 400000\n",
      "Extraction patche: 259000 / 400000\n",
      "Extraction patche: 260000 / 400000\n",
      "Extraction patche: 261000 / 400000\n",
      "Extraction patche: 262000 / 400000\n",
      "Extraction patche: 263000 / 400000\n",
      "Extraction patche: 264000 / 400000\n",
      "Extraction patche: 265000 / 400000\n",
      "Extraction patche: 266000 / 400000\n",
      "Extraction patche: 267000 / 400000\n",
      "Extraction patche: 268000 / 400000\n",
      "Extraction patche: 269000 / 400000\n",
      "Extraction patche: 270000 / 400000\n",
      "Extraction patche: 271000 / 400000\n",
      "Extraction patche: 272000 / 400000\n",
      "Extraction patche: 273000 / 400000\n",
      "Extraction patche: 274000 / 400000\n",
      "Extraction patche: 275000 / 400000\n",
      "Extraction patche: 276000 / 400000\n",
      "Extraction patche: 277000 / 400000\n",
      "Extraction patche: 278000 / 400000\n",
      "Extraction patche: 279000 / 400000\n",
      "Extraction patche: 280000 / 400000\n",
      "Extraction patche: 281000 / 400000\n",
      "Extraction patche: 282000 / 400000\n",
      "Extraction patche: 283000 / 400000\n",
      "Extraction patche: 284000 / 400000\n",
      "Extraction patche: 285000 / 400000\n",
      "Extraction patche: 286000 / 400000\n",
      "Extraction patche: 287000 / 400000\n",
      "Extraction patche: 288000 / 400000\n",
      "Extraction patche: 289000 / 400000\n",
      "Extraction patche: 290000 / 400000\n",
      "Extraction patche: 291000 / 400000\n",
      "Extraction patche: 292000 / 400000\n",
      "Extraction patche: 293000 / 400000\n",
      "Extraction patche: 294000 / 400000\n",
      "Extraction patche: 295000 / 400000\n",
      "Extraction patche: 296000 / 400000\n",
      "Extraction patche: 297000 / 400000\n",
      "Extraction patche: 298000 / 400000\n",
      "Extraction patche: 299000 / 400000\n",
      "Extraction patche: 300000 / 400000\n",
      "Extraction patche: 301000 / 400000\n",
      "Extraction patche: 302000 / 400000\n",
      "Extraction patche: 303000 / 400000\n",
      "Extraction patche: 304000 / 400000\n",
      "Extraction patche: 305000 / 400000\n",
      "Extraction patche: 306000 / 400000\n",
      "Extraction patche: 307000 / 400000\n",
      "Extraction patche: 308000 / 400000\n",
      "Extraction patche: 309000 / 400000\n",
      "Extraction patche: 310000 / 400000\n",
      "Extraction patche: 311000 / 400000\n",
      "Extraction patche: 312000 / 400000\n",
      "Extraction patche: 313000 / 400000\n",
      "Extraction patche: 314000 / 400000\n",
      "Extraction patche: 315000 / 400000\n",
      "Extraction patche: 316000 / 400000\n",
      "Extraction patche: 317000 / 400000\n",
      "Extraction patche: 318000 / 400000\n",
      "Extraction patche: 319000 / 400000\n",
      "Extraction patche: 320000 / 400000\n",
      "Extraction patche: 321000 / 400000\n",
      "Extraction patche: 322000 / 400000\n",
      "Extraction patche: 323000 / 400000\n",
      "Extraction patche: 324000 / 400000\n",
      "Extraction patche: 325000 / 400000\n",
      "Extraction patche: 326000 / 400000\n",
      "Extraction patche: 327000 / 400000\n",
      "Extraction patche: 328000 / 400000\n",
      "Extraction patche: 329000 / 400000\n",
      "Extraction patche: 330000 / 400000\n",
      "Extraction patche: 331000 / 400000\n",
      "Extraction patche: 332000 / 400000\n",
      "Extraction patche: 333000 / 400000\n",
      "Extraction patche: 334000 / 400000\n",
      "Extraction patche: 335000 / 400000\n",
      "Extraction patche: 336000 / 400000\n",
      "Extraction patche: 337000 / 400000\n",
      "Extraction patche: 338000 / 400000\n",
      "Extraction patche: 339000 / 400000\n",
      "Extraction patche: 340000 / 400000\n",
      "Extraction patche: 341000 / 400000\n",
      "Extraction patche: 342000 / 400000\n",
      "Extraction patche: 343000 / 400000\n",
      "Extraction patche: 344000 / 400000\n",
      "Extraction patche: 345000 / 400000\n",
      "Extraction patche: 346000 / 400000\n",
      "Extraction patche: 347000 / 400000\n",
      "Extraction patche: 348000 / 400000\n",
      "Extraction patche: 349000 / 400000\n",
      "Extraction patche: 350000 / 400000\n",
      "Extraction patche: 351000 / 400000\n",
      "Extraction patche: 352000 / 400000\n",
      "Extraction patche: 353000 / 400000\n",
      "Extraction patche: 354000 / 400000\n",
      "Extraction patche: 355000 / 400000\n",
      "Extraction patche: 356000 / 400000\n",
      "Extraction patche: 357000 / 400000\n",
      "Extraction patche: 358000 / 400000\n",
      "Extraction patche: 359000 / 400000\n",
      "Extraction patche: 360000 / 400000\n",
      "Extraction patche: 361000 / 400000\n",
      "Extraction patche: 362000 / 400000\n",
      "Extraction patche: 363000 / 400000\n",
      "Extraction patche: 364000 / 400000\n",
      "Extraction patche: 365000 / 400000\n",
      "Extraction patche: 366000 / 400000\n",
      "Extraction patche: 367000 / 400000\n",
      "Extraction patche: 368000 / 400000\n",
      "Extraction patche: 369000 / 400000\n",
      "Extraction patche: 370000 / 400000\n",
      "Extraction patche: 371000 / 400000\n",
      "Extraction patche: 372000 / 400000\n",
      "Extraction patche: 373000 / 400000\n",
      "Extraction patche: 374000 / 400000\n",
      "Extraction patche: 375000 / 400000\n",
      "Extraction patche: 376000 / 400000\n",
      "Extraction patche: 377000 / 400000\n",
      "Extraction patche: 378000 / 400000\n",
      "Extraction patche: 379000 / 400000\n",
      "Extraction patche: 380000 / 400000\n",
      "Extraction patche: 381000 / 400000\n",
      "Extraction patche: 382000 / 400000\n",
      "Extraction patche: 383000 / 400000\n",
      "Extraction patche: 384000 / 400000\n",
      "Extraction patche: 385000 / 400000\n",
      "Extraction patche: 386000 / 400000\n",
      "Extraction patche: 387000 / 400000\n",
      "Extraction patche: 388000 / 400000\n",
      "Extraction patche: 389000 / 400000\n",
      "Extraction patche: 390000 / 400000\n",
      "Extraction patche: 391000 / 400000\n",
      "Extraction patche: 392000 / 400000\n",
      "Extraction patche: 393000 / 400000\n",
      "Extraction patche: 394000 / 400000\n",
      "Extraction patche: 395000 / 400000\n",
      "Extraction patche: 396000 / 400000\n",
      "Extraction patche: 397000 / 400000\n",
      "Extraction patche: 398000 / 400000\n",
      "Extraction patche: 399000 / 400000\n",
      "[-1.63659378 -1.21653712 -0.65190373 -0.11309174  0.52777037  1.2240845\n",
      " -1.20338402 -0.40096427  0.57050453  0.96025422  1.17557147  1.5193643\n",
      " -0.37758391  0.4496897   1.03984608  1.58222308  1.6483154   1.69220657\n",
      "  0.54375173  1.12020515  1.25864676  1.50088511  1.82120413  1.83801724\n",
      "  1.33653536  1.58579841  1.298294    1.41383642  1.6844317   1.7288908\n",
      "  1.52533834  1.59759573  1.20927874  1.3755522   1.62259777  1.46540953\n",
      " -1.76567963 -1.56490896 -1.21940978 -0.89920796 -0.35028074  0.22785322\n",
      " -1.58145566 -1.07310553 -0.3694466  -0.07052693  0.1561178   0.48522569\n",
      " -1.10743752 -0.49691697 -0.14723477  0.28156998  0.3858097   0.46760458\n",
      " -0.51280126  0.05172009  0.02738303  0.23397182  0.54358755  0.57429877\n",
      "  0.2570677   0.54719799  0.12447777  0.23123907  0.44208263  0.45131964\n",
      "  0.42401751  0.56250713  0.0667396   0.25193161  0.44075266  0.22567188\n",
      " -1.6644659  -1.62853185 -1.49913003 -1.29410562 -0.9372108  -0.60015353\n",
      " -1.6805944  -1.43775663 -0.97458531 -0.71432881 -0.60371802 -0.41561618\n",
      " -1.48087027 -1.06024671 -0.9005307  -0.58675158 -0.57349315 -0.53189381\n",
      " -1.1607841  -0.60936163 -0.77315585 -0.6570677  -0.38829959 -0.37212049\n",
      " -0.6167276  -0.28922796 -0.72500629 -0.65911119 -0.46432592 -0.46913416\n",
      " -0.57497543 -0.27255838 -0.6807155  -0.56163233 -0.43930632 -0.7202822 ]\n"
     ]
    }
   ],
   "source": [
    "#create unsurpervised data\n",
    "import numpy as np\n",
    "patches=[]\n",
    "rfSize = 6\n",
    "numCentroids=600\n",
    "whitening=True\n",
    "numPatches = 400000\n",
    "CIFAR_DIM=[32,32,3]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "patches= np.zeros((numPatches, rfSize*rfSize*3))\n",
    "for i in range(0, numPatches):\n",
    "    if i%1000==0:\n",
    "        print('Extraction patche:', i, '/', numPatches)\n",
    "    r= np.random.randint(CIFAR_DIM[0]-rfSize) \n",
    "    c= np.random.randint(CIFAR_DIM[0]-rfSize)\n",
    "    patch= (X_train[(i%len(X_train)),:])  \n",
    "    \n",
    "    #print(patch.shape)#.reshape(32,32,3)\n",
    "    patch= patch[:,r:r+rfSize, c:c+rfSize]\n",
    "    \n",
    "    #print(patch.shape)\n",
    "    #print((patch.reshape(1,-1)).shape)\n",
    "    patches[i]=(patch.reshape(1,-1))\n",
    "patches=(patches-patches.mean(1)[:,None])/np.sqrt(patches.var(1)+10)[:,None]\n",
    "\n",
    "\n",
    "#from sklearn import preprocessing\n",
    "#patches = preprocessing.normalize(patches, norm='l2')\n",
    "\n",
    "#Normlize\n",
    "print(patches[0])\n",
    "[D,V]=np.linalg.eig(np.cov(patches,rowvar=0))\n",
    "\n",
    "P = V.dot(np.diag(np.sqrt(1/(D + 0.1)))).dot(V.T)\n",
    "patches = patches.dot(P)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can also use the run_kmeans I have code below \n",
    "\n",
    "kmeans = MiniBatchKMeans(n_clusters=1600, n_init=5, max_iter=100, init_size=4800, batch_size=1000).fit(patches)\n",
    "Nrep= kmeans.cluster_centers_\n",
    "NrepL= list(Nrep)\n",
    "print(len(Nrep))\n",
    "array= np.array(Nrep)\n",
    "centroids=array\n",
    "print(array.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def run_kmeans(X,k,iterations):\n",
    "    x2= np.sum(np.power(X,2), axis=1)\n",
    "   \n",
    "    centroids= np.random.randn(k, np.size(X,1))*0.1\n",
    "    \n",
    "    BATCH_SIZE=1000\n",
    "    \n",
    "    for itr in range(1,iterations+1):\n",
    "        print('kmeans iteration ', itr ,'/', iterations)\n",
    "        c2= 0.5*np.sum(np.power(centroids,2), axis=1)\n",
    "        summation= np.zeros((k, np.size(X,1))) # len(X[0][:])\n",
    "        #counts= np.zeros((k,1))\n",
    "        count1=[]\n",
    "        loss=0\n",
    "        c=0\n",
    "        for i in range(1,len(X), BATCH_SIZE ):\n",
    "            lastindex=min(i+BATCH_SIZE-1, len(X))\n",
    "            #print(lastindex)\n",
    "            m= lastindex -i +1 \n",
    "            #print(m)\n",
    "            batch= X[i-1:lastindex ,:]   #batch= X[i:lastindex+1 ,:]\n",
    "            batch_t=batch.transpose()\n",
    "            tmp= centroids.dot(batch_t)\n",
    "            for n in range(1, len(batch)):\n",
    "                tmp[:,n]=tmp[:,n]-c2\n",
    "                \n",
    "            val=[]\n",
    "            labels=[]\n",
    "            [val ,labels] = [np.max(tmp, axis=0),np.argmax(tmp, axis=0) ]   \n",
    "            \n",
    "            loss= loss + np.sum(0.5*x2[i-1:lastindex]-val.transpose())\n",
    "            #print(len(labels))\n",
    "            #print(np.max(labels))\n",
    "            \n",
    "            # tranformer label en matrice indicatrice\n",
    "            S=np.zeros((m,k))\n",
    "            #print(len(labels))\n",
    "            #print(lastindex-1-i)\n",
    "            for j in range(1, lastindex-i):\n",
    "                S[j][labels[j]]=1\n",
    "            #print(np.shape(S.transpose()))\n",
    "            #print(np.shape(X[i:lastindex+1,:]))\n",
    "            \n",
    "            summation = summation+(S.transpose()).dot(X[i-1:lastindex,:])\n",
    "            counts= (np.sum(S,axis=0).transpose())\n",
    "            count1.append(counts)\n",
    "            #print(np.shape(counts))\n",
    "            #print(np.shape(summation))\n",
    "            c=c+1\n",
    "            #print(c)\n",
    "            #normalize\n",
    "        counts=np.sum(np.array(count1),axis=0)\n",
    "        #print(np.shape(np.sum(S,axis=0).transpose()))\n",
    "       # print(np.shape(counts))\n",
    "        #print(np.shape(summation))    \n",
    "            #centroids = summation/counts\n",
    "        for i in range(1, k):\n",
    "            \n",
    "            if counts[i].all!=0:\n",
    "                centroids[i]=summation[i,:]/counts[i]\n",
    "            elif counts[i]==0:\n",
    "                centroids[i]=centroids[i]*0\n",
    "                    \n",
    "            \n",
    "        badIndex= [i for i,val in enumerate(counts) if val.any()==0]\n",
    "        centroids[badIndex,:]=0     \n",
    "            \n",
    "    return centroids, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kmeans iteration  1 / 100\n",
      "kmeans iteration  2 / 100\n",
      "kmeans iteration  3 / 100\n",
      "kmeans iteration  4 / 100\n",
      "kmeans iteration  5 / 100\n",
      "kmeans iteration  6 / 100\n",
      "kmeans iteration  7 / 100\n",
      "kmeans iteration  8 / 100\n",
      "kmeans iteration  9 / 100\n",
      "kmeans iteration  10 / 100\n",
      "kmeans iteration  11 / 100\n",
      "kmeans iteration  12 / 100\n",
      "kmeans iteration  13 / 100\n",
      "kmeans iteration  14 / 100\n",
      "kmeans iteration  15 / 100\n",
      "kmeans iteration  16 / 100\n",
      "kmeans iteration  17 / 100\n",
      "kmeans iteration  18 / 100\n",
      "kmeans iteration  19 / 100\n",
      "kmeans iteration  20 / 100\n",
      "kmeans iteration  21 / 100\n",
      "kmeans iteration  22 / 100\n",
      "kmeans iteration  23 / 100\n",
      "kmeans iteration  24 / 100\n",
      "kmeans iteration  25 / 100\n",
      "kmeans iteration  26 / 100\n",
      "kmeans iteration  27 / 100\n",
      "kmeans iteration  28 / 100\n",
      "kmeans iteration  29 / 100\n",
      "kmeans iteration  30 / 100\n",
      "kmeans iteration  31 / 100\n",
      "kmeans iteration  32 / 100\n",
      "kmeans iteration  33 / 100\n",
      "kmeans iteration  34 / 100\n",
      "kmeans iteration  35 / 100\n",
      "kmeans iteration  36 / 100\n",
      "kmeans iteration  37 / 100\n",
      "kmeans iteration  38 / 100\n",
      "kmeans iteration  39 / 100\n",
      "kmeans iteration  40 / 100\n",
      "kmeans iteration  41 / 100\n",
      "kmeans iteration  42 / 100\n",
      "kmeans iteration  43 / 100\n",
      "kmeans iteration  44 / 100\n",
      "kmeans iteration  45 / 100\n",
      "kmeans iteration  46 / 100\n",
      "kmeans iteration  47 / 100\n",
      "kmeans iteration  48 / 100\n",
      "kmeans iteration  49 / 100\n",
      "kmeans iteration  50 / 100\n",
      "kmeans iteration  51 / 100\n",
      "kmeans iteration  52 / 100\n",
      "kmeans iteration  53 / 100\n",
      "kmeans iteration  54 / 100\n",
      "kmeans iteration  55 / 100\n",
      "kmeans iteration  56 / 100\n",
      "kmeans iteration  57 / 100\n",
      "kmeans iteration  58 / 100\n",
      "kmeans iteration  59 / 100\n",
      "kmeans iteration  60 / 100\n",
      "kmeans iteration  61 / 100\n",
      "kmeans iteration  62 / 100\n",
      "kmeans iteration  63 / 100\n",
      "kmeans iteration  64 / 100\n",
      "kmeans iteration  65 / 100\n",
      "kmeans iteration  66 / 100\n",
      "kmeans iteration  67 / 100\n",
      "kmeans iteration  68 / 100\n",
      "kmeans iteration  69 / 100\n",
      "kmeans iteration  70 / 100\n",
      "kmeans iteration  71 / 100\n",
      "kmeans iteration  72 / 100\n",
      "kmeans iteration  73 / 100\n",
      "kmeans iteration  74 / 100\n",
      "kmeans iteration  75 / 100\n",
      "kmeans iteration  76 / 100\n",
      "kmeans iteration  77 / 100\n",
      "kmeans iteration  78 / 100\n",
      "kmeans iteration  79 / 100\n",
      "kmeans iteration  80 / 100\n",
      "kmeans iteration  81 / 100\n",
      "kmeans iteration  82 / 100\n",
      "kmeans iteration  83 / 100\n",
      "kmeans iteration  84 / 100\n",
      "kmeans iteration  85 / 100\n",
      "kmeans iteration  86 / 100\n",
      "kmeans iteration  87 / 100\n",
      "kmeans iteration  88 / 100\n",
      "kmeans iteration  89 / 100\n",
      "kmeans iteration  90 / 100\n",
      "kmeans iteration  91 / 100\n",
      "kmeans iteration  92 / 100\n",
      "kmeans iteration  93 / 100\n",
      "kmeans iteration  94 / 100\n",
      "kmeans iteration  95 / 100\n",
      "kmeans iteration  96 / 100\n",
      "kmeans iteration  97 / 100\n",
      "kmeans iteration  98 / 100\n",
      "kmeans iteration  99 / 100\n",
      "kmeans iteration  100 / 100\n"
     ]
    }
   ],
   "source": [
    "centroids=run_kmeans(patches,numCentroids,100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "centroids= np.array(centroids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function Im2col allows to find overllaping patch\n",
    "\n",
    "def im2col_sliding_broadcasting(A, BSZ, stepsize=1):\n",
    "    # Parameters\n",
    "    M,N = A.shape\n",
    "    col_extent = N - BSZ[1] + 1\n",
    "    row_extent = M - BSZ[0] + 1\n",
    "\n",
    "    # Get Starting block indices\n",
    "    start_idx = np.arange(BSZ[0])[:,None]*N + np.arange(BSZ[1])\n",
    "\n",
    "    # Get offsetted indices across the height and width of input array\n",
    "    offset_idx = np.arange(row_extent)[:,None]*N + np.arange(col_extent)\n",
    "\n",
    "    # Get all actual indices & index into input array for final output\n",
    "    return np.take (A,start_idx.ravel()[:,None] + offset_idx.ravel()[::stepsize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial import distance_matrix\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from scipy.spatial import distance\n",
    "#XC = np.zeros((50000,4*100))\n",
    "def extract_features (X):\n",
    "    whitening = True #(nargin==6)\n",
    "    trainXC=[]\n",
    "    liste=[]\n",
    "    for i in range(0, len(X)):\n",
    "        if (i%1000==0):\n",
    "            print('Extraction features: ')#+i+ '/'+ len(X))\n",
    "            \n",
    "        #shape de patches (729,108)    \n",
    "        patches =np.concatenate([im2col_sliding_broadcasting((X[i].reshape(-1,1)[0:1024]).reshape(32,32) , [6,6], stepsize=1)\n",
    "        ,im2col_sliding_broadcasting((X[i].reshape(-1,1)[1024:2048]).reshape(32,32), [6,6], stepsize=1),\n",
    "        im2col_sliding_broadcasting((X[i].reshape(-1,1)[2048:]).reshape(32,32),[ 6,6], stepsize=1) ], axis=0)\n",
    "        patches=patches.transpose()\n",
    "        \n",
    "        patches=np.array(patches)\n",
    "        \n",
    "        \n",
    "        patches=(patches-patches.mean(1)[:,None])/(np.sqrt(patches.var(1)+10)[:,None])\n",
    "        #map to feature space\n",
    "        patches=patches.dot(P)\n",
    "        #calculate distance using x2-2xc+c2\n",
    "        x2=np.power(patches,2).sum(1)\n",
    "        c2=np.power(centroids,2).sum(1)\n",
    "        xc=patches.dot(centroids.T)\n",
    "        dist=np.sqrt(-2*xc+x2[:,None]+c2)\n",
    "        \n",
    "       #patches.dot(P) si matrice\n",
    "      \n",
    "        \n",
    "        \n",
    "        z =dist # 13h          \n",
    "      \n",
    "        \n",
    "        u=dist.mean(1)\n",
    "        patches=np.maximum(-dist+u[:,None],0)\n",
    "        rs=CIFAR_DIM[0]-rfSize+1\n",
    "        cs=CIFAR_DIM[1]-rfSize+1\n",
    "        patches=np.reshape(patches,[rs,cs,-1])\n",
    "        q=[]\n",
    "        q.append(patches[0:int(rs/2)+1,0:int(cs/2)+1].sum(0).sum(0))\n",
    "        q.append(patches[0:int(rs/2)+1,int(cs/2)+1:cs-1].sum(0).sum(0))\n",
    "        q.append(patches[int(rs/2)+1:rs-1,0:int(cs/2)+1].sum(0).sum(0))\n",
    "        q.append(patches[int(rs/2)+1:rs-1,int(cs/2)+1:cs-1].sum(0).sum(0))\n",
    "        q=np.array(q).ravel()\n",
    "        trainXC.append(q)\n",
    "    trainXC=np.array(trainXC)\n",
    "    trainXC=(trainXC-trainXC.mean(1)[:,None])/(np.sqrt(trainXC.var(1)+.01)[:,None])\n",
    "    return trainXC\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction features: \n",
      "Extraction features: \n",
      "Extraction features: \n",
      "Extraction features: \n",
      "Extraction features: \n",
      "Extraction features: \n",
      "Extraction features: \n",
      "Extraction features: \n",
      "Extraction features: \n",
      "Extraction features: \n",
      "Extraction features: \n",
      "Extraction features: \n",
      "Extraction features: \n",
      "Extraction features: \n",
      "Extraction features: \n",
      "Extraction features: \n",
      "Extraction features: \n",
      "Extraction features: \n",
      "Extraction features: \n",
      "Extraction features: \n",
      "Extraction features: \n",
      "Extraction features: \n",
      "Extraction features: \n",
      "Extraction features: \n",
      "Extraction features: \n",
      "Extraction features: \n",
      "Extraction features: \n",
      "Extraction features: \n",
      "Extraction features: \n",
      "Extraction features: \n",
      "Extraction features: \n",
      "Extraction features: \n",
      "Extraction features: \n",
      "Extraction features: \n",
      "Extraction features: \n",
      "Extraction features: \n",
      "Extraction features: \n",
      "Extraction features: \n",
      "Extraction features: \n",
      "Extraction features: \n",
      "Extraction features: \n",
      "Extraction features: \n",
      "Extraction features: \n",
      "Extraction features: \n",
      "Extraction features: \n",
      "Extraction features: \n",
      "Extraction features: \n",
      "Extraction features: \n",
      "Extraction features: \n",
      "(2400,)\n"
     ]
    }
   ],
   "source": [
    "### Calcul of features , with matrices distances ,take distance < mean(distance) + pooling patches\n",
    "\n",
    "#Extract features of Xtrain\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "A=extract_features (X_train )\n",
    "\n",
    "print(np.shape(A[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49000, 2400)\n",
      "Extraction features: \n"
     ]
    }
   ],
   "source": [
    "#Extract features of X_test \n",
    "print(A.shape)\n",
    "\n",
    "BBB=extract_features (X_val )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train svm classifier \n",
    "import time \n",
    "tabLoss=[]\n",
    "tabAccT=[]\n",
    "tabAccTest=[]\n",
    "iterations=0\n",
    "import numpy as np\n",
    "def my_l2svmloss(w, *args):\n",
    "    #print(len(args))\n",
    "    start_time=time.time()\n",
    "    X=args[0]\n",
    "    y=args[1]\n",
    "    K=args[2]\n",
    "    C=args[3]\n",
    "    X_test=args[4]\n",
    "    y_test=args[5]\n",
    "    \n",
    "    [M,N]= np.shape(X)   \n",
    "    theta= w.reshape(N,K)  \n",
    "    Y= np.zeros((len(y),K))   \n",
    "    for i in range(0, len(y)):\n",
    "        for k in range(1, K+1):\n",
    "            if y[i]==k:\n",
    "                Y[i,k-1]=Y[i,k-1]+1       \n",
    "            else: Y[i,k-1]=Y[i,k-1]-1\n",
    "\n",
    "    margin = np.maximum(0, 1-Y*(X.dot(theta)))    \n",
    "  \n",
    "    loss= (0.5* np.sum((np.power(theta,2)), axis=0))+ C*np.mean((np.power(margin,2)), axis=0)\n",
    "  \n",
    "    loss= np.sum(loss, axis=0) \n",
    "    tabLoss.append(loss)\n",
    "    g = theta -2*C/M*(X.transpose().dot(margin*Y))\n",
    "    g.reshape(np.size(X, 1)*K,1)\n",
    "    \n",
    "    #ajout du calcul accuracy X_train et X_val\n",
    "    thetaBis= w.reshape(np.size(X, 1), K )\n",
    "    MatriceB = X.dot(thetaBis)      \n",
    "    MatriceVal= X_test.dot(thetaBis)\n",
    "    [val1 ,inds1] = [np.max(MatriceB, axis=1),np.argmax(MatriceB, axis=1) ]  \n",
    "    [val2 ,inds2] = [np.max(MatriceVal, axis=1),np.argmax(MatriceVal, axis=1) ] \n",
    "    temps= time.time() - start_time\n",
    "    global iterations\n",
    "    iterations= iterations+1\n",
    "    print('Iterations: ', iterations, 'train result : ', round((inds1==y_train).mean(),4), 'test result: ', (inds2==y_test).mean(), 'loss: ', round(loss,3), 'temps calcul: ' , round(temps,4))\n",
    "    tabAccT.append((inds1==y_train).mean())\n",
    "    tabAccTest.append((inds2==y_test).mean())\n",
    "   \n",
    "  \n",
    "    return loss, np.hstack(g)\n",
    "\n",
    "#objectif mininimiser loss et g \n",
    "\n",
    "\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "def train_svm(trainXC, trainY, C, X_test, y_test):\n",
    "    print(\"Mille itérations maximum\")\n",
    "    X= trainXC   #featurize data \n",
    "    y= trainY   \n",
    "    \n",
    "    K = np.max(trainY)  #number of classes \n",
    "    \n",
    "    w0= np.zeros((np.size(X, 1)*K,1) )# 10* (4*numcentroids+1) , w vector of weight for the 10 classes \n",
    "    w0=np.array(w0)\n",
    "    #minimize the function svmloss\n",
    "    iterations=1\n",
    "   \n",
    "    w, fw ,i   = fmin_l_bfgs_b(my_l2svmloss,w0,args =([X,y,K,C , X_test, y_test ]), maxfun=1000, maxiter=1000)  #x,f,d   / w, fw, i\n",
    "    print('nombre total d iterations' , i['nit'])\n",
    "    theta= w.reshape(np.size(trainXC, 1), K )\n",
    "    return theta ,i\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction features: \n",
      "Mille itérations maximum\n",
      "Iterations:  1 train result :  0.1003 test result:  0.103 loss:  1000.0 temps calcul:  0.738\n",
      "Iterations:  2 train result :  0.4154 test result:  0.411 loss:  7656.591 temps calcul:  0.755\n",
      "Iterations:  3 train result :  0.4154 test result:  0.411 loss:  955.969 temps calcul:  0.8129\n",
      "Iterations:  4 train result :  0.4651 test result:  0.466 loss:  925.179 temps calcul:  0.745\n",
      "Iterations:  5 train result :  0.5049 test result:  0.506 loss:  898.331 temps calcul:  0.745\n",
      "Iterations:  6 train result :  0.5214 test result:  0.519 loss:  864.046 temps calcul:  0.734\n",
      "Iterations:  7 train result :  0.5268 test result:  0.531 loss:  802.928 temps calcul:  0.745\n",
      "Iterations:  8 train result :  0.4854 test result:  0.472 loss:  729.182 temps calcul:  0.737\n",
      "Iterations:  9 train result :  0.5552 test result:  0.556 loss:  608.329 temps calcul:  0.734\n",
      "Iterations:  10 train result :  0.5686 test result:  0.559 loss:  513.698 temps calcul:  0.8392\n",
      "Iterations:  11 train result :  0.564 test result:  0.562 loss:  406.189 temps calcul:  0.753\n",
      "Iterations:  12 train result :  0.5287 test result:  0.514 loss:  358.756 temps calcul:  0.8182\n",
      "Iterations:  13 train result :  0.5632 test result:  0.538 loss:  303.537 temps calcul:  0.752\n",
      "Iterations:  14 train result :  0.5802 test result:  0.568 loss:  279.973 temps calcul:  0.74\n",
      "Iterations:  15 train result :  0.5942 test result:  0.579 loss:  261.981 temps calcul:  0.742\n",
      "Iterations:  16 train result :  0.608 test result:  0.595 loss:  245.654 temps calcul:  0.7309\n",
      "Iterations:  17 train result :  0.5884 test result:  0.593 loss:  250.636 temps calcul:  0.7309\n",
      "Iterations:  18 train result :  0.6184 test result:  0.6 loss:  239.519 temps calcul:  0.8302\n",
      "Iterations:  19 train result :  0.6266 test result:  0.592 loss:  233.179 temps calcul:  0.755\n",
      "Iterations:  20 train result :  0.6341 test result:  0.604 loss:  229.545 temps calcul:  0.7537\n",
      "Iterations:  21 train result :  0.6416 test result:  0.614 loss:  226.625 temps calcul:  0.756\n",
      "Iterations:  22 train result :  0.6469 test result:  0.617 loss:  224.169 temps calcul:  0.738\n",
      "Iterations:  23 train result :  0.6557 test result:  0.625 loss:  221.925 temps calcul:  0.7289\n",
      "Iterations:  24 train result :  0.6589 test result:  0.631 loss:  220.765 temps calcul:  0.7279\n",
      "Iterations:  25 train result :  0.6627 test result:  0.635 loss:  219.186 temps calcul:  0.7319\n",
      "Iterations:  26 train result :  0.6643 test result:  0.644 loss:  218.219 temps calcul:  0.743\n",
      "Iterations:  27 train result :  0.6683 test result:  0.646 loss:  217.053 temps calcul:  0.7299\n",
      "Iterations:  28 train result :  0.6698 test result:  0.646 loss:  216.027 temps calcul:  0.7319\n",
      "Iterations:  29 train result :  0.6724 test result:  0.648 loss:  215.244 temps calcul:  0.739\n",
      "Iterations:  30 train result :  0.6761 test result:  0.654 loss:  213.768 temps calcul:  0.8473\n",
      "Iterations:  31 train result :  0.6803 test result:  0.655 loss:  212.622 temps calcul:  0.8142\n",
      "Iterations:  32 train result :  0.6826 test result:  0.657 loss:  211.816 temps calcul:  0.755\n",
      "Iterations:  33 train result :  0.6842 test result:  0.661 loss:  211.015 temps calcul:  0.757\n",
      "Iterations:  34 train result :  0.6863 test result:  0.663 loss:  210.14 temps calcul:  0.75\n",
      "Iterations:  35 train result :  0.6887 test result:  0.66 loss:  208.844 temps calcul:  0.735\n",
      "Iterations:  36 train result :  0.6923 test result:  0.661 loss:  207.649 temps calcul:  0.7319\n",
      "Iterations:  37 train result :  0.6939 test result:  0.661 loss:  206.343 temps calcul:  0.7279\n",
      "Iterations:  38 train result :  0.6963 test result:  0.664 loss:  205.218 temps calcul:  0.7299\n",
      "Iterations:  39 train result :  0.7022 test result:  0.668 loss:  203.883 temps calcul:  0.7289\n",
      "Iterations:  40 train result :  0.7042 test result:  0.673 loss:  202.754 temps calcul:  0.7279\n",
      "Iterations:  41 train result :  0.7073 test result:  0.673 loss:  201.849 temps calcul:  0.7309\n",
      "Iterations:  42 train result :  0.7097 test result:  0.681 loss:  201.045 temps calcul:  0.7259\n",
      "Iterations:  43 train result :  0.7113 test result:  0.685 loss:  200.203 temps calcul:  0.7279\n",
      "Iterations:  44 train result :  0.7133 test result:  0.683 loss:  199.257 temps calcul:  0.7219\n",
      "Iterations:  45 train result :  0.7149 test result:  0.683 loss:  198.481 temps calcul:  0.7229\n",
      "Iterations:  46 train result :  0.7153 test result:  0.687 loss:  197.801 temps calcul:  0.7279\n",
      "Iterations:  47 train result :  0.7167 test result:  0.693 loss:  197.152 temps calcul:  0.7319\n",
      "Iterations:  48 train result :  0.718 test result:  0.696 loss:  196.381 temps calcul:  0.736\n",
      "Iterations:  49 train result :  0.7196 test result:  0.695 loss:  195.949 temps calcul:  0.7309\n",
      "Iterations:  50 train result :  0.72 test result:  0.696 loss:  195.418 temps calcul:  0.7219\n",
      "Iterations:  51 train result :  0.7212 test result:  0.691 loss:  194.968 temps calcul:  0.8302\n",
      "Iterations:  52 train result :  0.7223 test result:  0.691 loss:  194.439 temps calcul:  0.8643\n",
      "Iterations:  53 train result :  0.7232 test result:  0.693 loss:  193.777 temps calcul:  0.7741\n",
      "Iterations:  54 train result :  0.7236 test result:  0.691 loss:  193.146 temps calcul:  0.8352\n",
      "Iterations:  55 train result :  0.7247 test result:  0.69 loss:  192.739 temps calcul:  0.77\n",
      "Iterations:  56 train result :  0.7266 test result:  0.691 loss:  192.249 temps calcul:  0.8422\n",
      "Iterations:  57 train result :  0.7271 test result:  0.691 loss:  191.878 temps calcul:  0.738\n",
      "Iterations:  58 train result :  0.7274 test result:  0.692 loss:  191.398 temps calcul:  0.761\n",
      "Iterations:  59 train result :  0.7287 test result:  0.694 loss:  190.889 temps calcul:  0.741\n",
      "Iterations:  60 train result :  0.7302 test result:  0.693 loss:  190.527 temps calcul:  0.744\n",
      "Iterations:  61 train result :  0.731 test result:  0.692 loss:  190.244 temps calcul:  0.736\n",
      "Iterations:  62 train result :  0.7331 test result:  0.7 loss:  189.585 temps calcul:  0.735\n",
      "Iterations:  63 train result :  0.7342 test result:  0.705 loss:  189.285 temps calcul:  0.733\n",
      "Iterations:  64 train result :  0.7347 test result:  0.703 loss:  188.917 temps calcul:  0.8432\n",
      "Iterations:  65 train result :  0.7349 test result:  0.699 loss:  188.4 temps calcul:  0.743\n",
      "Iterations:  66 train result :  0.7364 test result:  0.697 loss:  188.114 temps calcul:  0.7309\n",
      "Iterations:  67 train result :  0.737 test result:  0.694 loss:  187.655 temps calcul:  0.735\n",
      "Iterations:  68 train result :  0.7387 test result:  0.696 loss:  187.312 temps calcul:  0.733\n",
      "Iterations:  69 train result :  0.7393 test result:  0.693 loss:  187.032 temps calcul:  0.736\n",
      "Iterations:  70 train result :  0.7407 test result:  0.695 loss:  186.704 temps calcul:  0.768\n",
      "Iterations:  71 train result :  0.7409 test result:  0.7 loss:  186.395 temps calcul:  0.8071\n",
      "Iterations:  72 train result :  0.7419 test result:  0.7 loss:  186.037 temps calcul:  0.737\n",
      "Iterations:  73 train result :  0.7431 test result:  0.699 loss:  185.767 temps calcul:  0.7319\n",
      "Iterations:  74 train result :  0.7445 test result:  0.7 loss:  185.512 temps calcul:  0.7249\n",
      "Iterations:  75 train result :  0.7457 test result:  0.697 loss:  185.222 temps calcul:  0.8342\n",
      "Iterations:  76 train result :  0.7463 test result:  0.697 loss:  184.991 temps calcul:  0.74\n",
      "Iterations:  77 train result :  0.7473 test result:  0.702 loss:  184.735 temps calcul:  0.743\n",
      "Iterations:  78 train result :  0.7481 test result:  0.703 loss:  184.525 temps calcul:  0.736\n",
      "Iterations:  79 train result :  0.7478 test result:  0.704 loss:  184.292 temps calcul:  0.747\n",
      "Iterations:  80 train result :  0.7503 test result:  0.71 loss:  184.125 temps calcul:  0.735\n",
      "Iterations:  81 train result :  0.75 test result:  0.708 loss:  183.801 temps calcul:  0.8312\n",
      "Iterations:  82 train result :  0.7508 test result:  0.706 loss:  183.659 temps calcul:  0.737\n",
      "Iterations:  83 train result :  0.7513 test result:  0.706 loss:  183.416 temps calcul:  0.748\n",
      "Iterations:  84 train result :  0.7511 test result:  0.709 loss:  183.221 temps calcul:  0.736\n",
      "Iterations:  85 train result :  0.7522 test result:  0.71 loss:  182.946 temps calcul:  0.8473\n",
      "Iterations:  86 train result :  0.7523 test result:  0.71 loss:  182.773 temps calcul:  0.8352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations:  87 train result :  0.7533 test result:  0.707 loss:  182.577 temps calcul:  0.753\n",
      "Iterations:  88 train result :  0.7528 test result:  0.708 loss:  182.566 temps calcul:  0.7941\n",
      "Iterations:  89 train result :  0.7535 test result:  0.706 loss:  182.436 temps calcul:  0.8392\n",
      "Iterations:  90 train result :  0.7538 test result:  0.71 loss:  182.229 temps calcul:  0.741\n",
      "Iterations:  91 train result :  0.754 test result:  0.714 loss:  182.015 temps calcul:  0.7279\n",
      "Iterations:  92 train result :  0.7543 test result:  0.711 loss:  181.855 temps calcul:  0.7239\n",
      "Iterations:  93 train result :  0.7548 test result:  0.709 loss:  181.684 temps calcul:  0.7249\n",
      "Iterations:  94 train result :  0.7557 test result:  0.709 loss:  181.509 temps calcul:  0.7269\n",
      "Iterations:  95 train result :  0.7569 test result:  0.709 loss:  181.319 temps calcul:  0.737\n",
      "Iterations:  96 train result :  0.7572 test result:  0.71 loss:  181.15 temps calcul:  0.736\n",
      "Iterations:  97 train result :  0.7569 test result:  0.71 loss:  180.983 temps calcul:  0.8382\n",
      "Iterations:  98 train result :  0.7578 test result:  0.711 loss:  180.803 temps calcul:  0.769\n",
      "Iterations:  99 train result :  0.7588 test result:  0.708 loss:  180.623 temps calcul:  0.7891\n",
      "Iterations:  100 train result :  0.7591 test result:  0.709 loss:  180.484 temps calcul:  0.8152\n",
      "Iterations:  101 train result :  0.7606 test result:  0.709 loss:  180.294 temps calcul:  0.8182\n",
      "Iterations:  102 train result :  0.761 test result:  0.709 loss:  180.187 temps calcul:  0.7781\n",
      "Iterations:  103 train result :  0.761 test result:  0.706 loss:  180.053 temps calcul:  0.7981\n",
      "Iterations:  104 train result :  0.7607 test result:  0.709 loss:  179.92 temps calcul:  0.8593\n",
      "Iterations:  105 train result :  0.7612 test result:  0.709 loss:  179.832 temps calcul:  0.8503\n",
      "Iterations:  106 train result :  0.7625 test result:  0.713 loss:  179.648 temps calcul:  0.8473\n",
      "Iterations:  107 train result :  0.7625 test result:  0.714 loss:  179.483 temps calcul:  0.8513\n",
      "Iterations:  108 train result :  0.7626 test result:  0.714 loss:  179.388 temps calcul:  0.738\n",
      "Iterations:  109 train result :  0.7627 test result:  0.712 loss:  179.279 temps calcul:  0.8593\n",
      "Iterations:  110 train result :  0.7632 test result:  0.714 loss:  179.18 temps calcul:  0.8422\n",
      "Iterations:  111 train result :  0.7637 test result:  0.714 loss:  179.05 temps calcul:  0.7991\n",
      "Iterations:  112 train result :  0.7649 test result:  0.711 loss:  178.928 temps calcul:  0.753\n",
      "Iterations:  113 train result :  0.7648 test result:  0.711 loss:  178.828 temps calcul:  0.75\n",
      "Iterations:  114 train result :  0.7646 test result:  0.71 loss:  178.749 temps calcul:  0.8673\n",
      "Iterations:  115 train result :  0.7657 test result:  0.714 loss:  178.634 temps calcul:  0.757\n",
      "Iterations:  116 train result :  0.7659 test result:  0.716 loss:  178.521 temps calcul:  0.734\n",
      "Iterations:  117 train result :  0.7659 test result:  0.715 loss:  178.457 temps calcul:  0.738\n",
      "Iterations:  118 train result :  0.7667 test result:  0.711 loss:  178.324 temps calcul:  0.75\n",
      "Iterations:  119 train result :  0.7677 test result:  0.713 loss:  178.273 temps calcul:  0.7289\n",
      "Iterations:  120 train result :  0.7676 test result:  0.715 loss:  178.195 temps calcul:  0.73\n",
      "Iterations:  121 train result :  0.768 test result:  0.716 loss:  178.114 temps calcul:  0.7209\n",
      "Iterations:  122 train result :  0.7683 test result:  0.716 loss:  178.051 temps calcul:  0.7269\n",
      "Iterations:  123 train result :  0.7691 test result:  0.715 loss:  177.932 temps calcul:  0.7299\n",
      "Iterations:  124 train result :  0.7694 test result:  0.715 loss:  177.828 temps calcul:  0.7259\n",
      "Iterations:  125 train result :  0.7697 test result:  0.714 loss:  177.756 temps calcul:  0.7169\n",
      "Iterations:  126 train result :  0.7698 test result:  0.714 loss:  177.664 temps calcul:  0.7279\n",
      "Iterations:  127 train result :  0.7705 test result:  0.713 loss:  177.579 temps calcul:  0.7259\n",
      "Iterations:  128 train result :  0.7712 test result:  0.711 loss:  177.471 temps calcul:  0.7199\n",
      "Iterations:  129 train result :  0.7714 test result:  0.713 loss:  177.396 temps calcul:  0.7179\n",
      "Iterations:  130 train result :  0.7713 test result:  0.714 loss:  177.33 temps calcul:  0.7329\n",
      "Iterations:  131 train result :  0.7709 test result:  0.712 loss:  177.26 temps calcul:  0.7309\n",
      "Iterations:  132 train result :  0.7708 test result:  0.712 loss:  177.208 temps calcul:  0.736\n",
      "Iterations:  133 train result :  0.7709 test result:  0.713 loss:  177.149 temps calcul:  0.7199\n",
      "Iterations:  134 train result :  0.7714 test result:  0.71 loss:  177.101 temps calcul:  0.7239\n",
      "Iterations:  135 train result :  0.7718 test result:  0.712 loss:  177.044 temps calcul:  0.7219\n",
      "Iterations:  136 train result :  0.7729 test result:  0.715 loss:  176.964 temps calcul:  0.7249\n",
      "Iterations:  137 train result :  0.7731 test result:  0.714 loss:  176.891 temps calcul:  0.7259\n",
      "Iterations:  138 train result :  0.7731 test result:  0.712 loss:  176.832 temps calcul:  0.7279\n",
      "Iterations:  139 train result :  0.7731 test result:  0.71 loss:  176.758 temps calcul:  0.7259\n",
      "Iterations:  140 train result :  0.7734 test result:  0.71 loss:  176.704 temps calcul:  0.7209\n",
      "Iterations:  141 train result :  0.7739 test result:  0.708 loss:  176.631 temps calcul:  0.7249\n",
      "Iterations:  142 train result :  0.7744 test result:  0.715 loss:  176.568 temps calcul:  0.7219\n",
      "Iterations:  143 train result :  0.7753 test result:  0.714 loss:  176.514 temps calcul:  0.7249\n",
      "Iterations:  144 train result :  0.7754 test result:  0.713 loss:  176.465 temps calcul:  0.737\n",
      "Iterations:  145 train result :  0.7759 test result:  0.717 loss:  176.399 temps calcul:  0.749\n",
      "Iterations:  146 train result :  0.7763 test result:  0.72 loss:  176.352 temps calcul:  0.7279\n",
      "Iterations:  147 train result :  0.7764 test result:  0.719 loss:  176.31 temps calcul:  0.7209\n",
      "Iterations:  148 train result :  0.7766 test result:  0.717 loss:  176.241 temps calcul:  0.7319\n",
      "Iterations:  149 train result :  0.7764 test result:  0.715 loss:  176.2 temps calcul:  0.749\n",
      "Iterations:  150 train result :  0.7767 test result:  0.715 loss:  176.145 temps calcul:  0.8533\n",
      "Iterations:  151 train result :  0.7766 test result:  0.717 loss:  176.093 temps calcul:  0.7249\n",
      "Iterations:  152 train result :  0.7771 test result:  0.72 loss:  176.047 temps calcul:  0.7279\n",
      "Iterations:  153 train result :  0.7772 test result:  0.718 loss:  176.008 temps calcul:  0.7289\n",
      "Iterations:  154 train result :  0.7773 test result:  0.716 loss:  175.968 temps calcul:  0.8202\n",
      "Iterations:  155 train result :  0.7777 test result:  0.717 loss:  175.931 temps calcul:  0.745\n",
      "Iterations:  156 train result :  0.7771 test result:  0.715 loss:  175.9 temps calcul:  0.746\n",
      "Iterations:  157 train result :  0.7774 test result:  0.714 loss:  175.842 temps calcul:  0.736\n",
      "Iterations:  158 train result :  0.7778 test result:  0.718 loss:  175.815 temps calcul:  0.733\n",
      "Iterations:  159 train result :  0.7777 test result:  0.717 loss:  175.786 temps calcul:  0.8954\n",
      "Iterations:  160 train result :  0.7779 test result:  0.718 loss:  175.734 temps calcul:  0.738\n",
      "Iterations:  161 train result :  0.7782 test result:  0.718 loss:  175.731 temps calcul:  0.7289\n",
      "Iterations:  162 train result :  0.7781 test result:  0.717 loss:  175.696 temps calcul:  0.736\n",
      "Iterations:  163 train result :  0.7782 test result:  0.715 loss:  175.656 temps calcul:  0.733\n",
      "Iterations:  164 train result :  0.7782 test result:  0.716 loss:  175.625 temps calcul:  0.743\n",
      "Iterations:  165 train result :  0.7785 test result:  0.716 loss:  175.587 temps calcul:  0.7309\n",
      "Iterations:  166 train result :  0.7785 test result:  0.714 loss:  175.557 temps calcul:  0.737\n",
      "Iterations:  167 train result :  0.7787 test result:  0.713 loss:  175.514 temps calcul:  0.7309\n",
      "Iterations:  168 train result :  0.7785 test result:  0.713 loss:  175.491 temps calcul:  0.7299\n",
      "Iterations:  169 train result :  0.7785 test result:  0.712 loss:  175.468 temps calcul:  0.7289\n",
      "Iterations:  170 train result :  0.7787 test result:  0.714 loss:  175.439 temps calcul:  0.7279\n",
      "Iterations:  171 train result :  0.7786 test result:  0.714 loss:  175.402 temps calcul:  0.737\n",
      "Iterations:  172 train result :  0.7787 test result:  0.716 loss:  175.379 temps calcul:  0.734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations:  173 train result :  0.779 test result:  0.715 loss:  175.349 temps calcul:  0.7299\n",
      "Iterations:  174 train result :  0.7791 test result:  0.716 loss:  175.337 temps calcul:  0.734\n",
      "Iterations:  175 train result :  0.7794 test result:  0.715 loss:  175.303 temps calcul:  0.736\n",
      "Iterations:  176 train result :  0.7794 test result:  0.717 loss:  175.281 temps calcul:  0.7259\n",
      "Iterations:  177 train result :  0.7797 test result:  0.717 loss:  175.261 temps calcul:  0.733\n",
      "Iterations:  178 train result :  0.7798 test result:  0.716 loss:  175.227 temps calcul:  0.736\n",
      "Iterations:  179 train result :  0.7799 test result:  0.715 loss:  175.264 temps calcul:  0.8352\n",
      "Iterations:  180 train result :  0.7798 test result:  0.717 loss:  175.21 temps calcul:  0.741\n",
      "Iterations:  181 train result :  0.7804 test result:  0.717 loss:  175.182 temps calcul:  0.7771\n",
      "Iterations:  182 train result :  0.7803 test result:  0.718 loss:  175.165 temps calcul:  0.746\n",
      "Iterations:  183 train result :  0.7806 test result:  0.719 loss:  175.135 temps calcul:  0.737\n",
      "Iterations:  184 train result :  0.7808 test result:  0.721 loss:  175.126 temps calcul:  0.7319\n",
      "Iterations:  185 train result :  0.7806 test result:  0.72 loss:  175.104 temps calcul:  0.7284\n",
      "Iterations:  186 train result :  0.7806 test result:  0.719 loss:  175.091 temps calcul:  0.7259\n",
      "Iterations:  187 train result :  0.7809 test result:  0.721 loss:  175.076 temps calcul:  0.7229\n",
      "Iterations:  188 train result :  0.7809 test result:  0.721 loss:  175.049 temps calcul:  0.7239\n",
      "Iterations:  189 train result :  0.7811 test result:  0.722 loss:  175.102 temps calcul:  0.7309\n",
      "Iterations:  190 train result :  0.7808 test result:  0.72 loss:  175.035 temps calcul:  0.7239\n",
      "Iterations:  191 train result :  0.7811 test result:  0.722 loss:  175.009 temps calcul:  0.7239\n",
      "Iterations:  192 train result :  0.7815 test result:  0.722 loss:  174.989 temps calcul:  0.7259\n",
      "Iterations:  193 train result :  0.7816 test result:  0.721 loss:  174.968 temps calcul:  0.7319\n",
      "Iterations:  194 train result :  0.7817 test result:  0.723 loss:  174.956 temps calcul:  0.7239\n",
      "Iterations:  195 train result :  0.782 test result:  0.721 loss:  174.939 temps calcul:  0.7239\n",
      "Iterations:  196 train result :  0.7817 test result:  0.72 loss:  174.925 temps calcul:  0.7229\n",
      "Iterations:  197 train result :  0.7819 test result:  0.722 loss:  174.913 temps calcul:  0.7239\n",
      "Iterations:  198 train result :  0.782 test result:  0.721 loss:  174.89 temps calcul:  0.7209\n",
      "Iterations:  199 train result :  0.782 test result:  0.722 loss:  174.872 temps calcul:  0.7229\n",
      "Iterations:  200 train result :  0.782 test result:  0.722 loss:  174.857 temps calcul:  0.7279\n",
      "Iterations:  201 train result :  0.7822 test result:  0.721 loss:  174.846 temps calcul:  0.7199\n",
      "Iterations:  202 train result :  0.7821 test result:  0.721 loss:  174.834 temps calcul:  0.753\n",
      "Iterations:  203 train result :  0.782 test result:  0.72 loss:  174.826 temps calcul:  0.737\n",
      "Iterations:  204 train result :  0.7821 test result:  0.72 loss:  174.806 temps calcul:  0.8051\n",
      "Iterations:  205 train result :  0.7822 test result:  0.721 loss:  174.8 temps calcul:  0.767\n",
      "Iterations:  206 train result :  0.7819 test result:  0.721 loss:  174.79 temps calcul:  0.7931\n",
      "Iterations:  207 train result :  0.782 test result:  0.722 loss:  174.777 temps calcul:  0.745\n",
      "Iterations:  208 train result :  0.782 test result:  0.722 loss:  174.763 temps calcul:  0.738\n",
      "Iterations:  209 train result :  0.7821 test result:  0.723 loss:  174.753 temps calcul:  0.74\n",
      "Iterations:  210 train result :  0.7824 test result:  0.722 loss:  174.742 temps calcul:  0.7319\n",
      "Iterations:  211 train result :  0.7827 test result:  0.721 loss:  174.734 temps calcul:  0.748\n",
      "Iterations:  212 train result :  0.7827 test result:  0.721 loss:  174.725 temps calcul:  0.738\n",
      "Iterations:  213 train result :  0.7832 test result:  0.723 loss:  174.71 temps calcul:  0.7289\n",
      "Iterations:  214 train result :  0.7831 test result:  0.721 loss:  174.703 temps calcul:  0.7239\n",
      "Iterations:  215 train result :  0.783 test result:  0.722 loss:  174.689 temps calcul:  0.7289\n",
      "Iterations:  216 train result :  0.7826 test result:  0.721 loss:  174.685 temps calcul:  0.7229\n",
      "Iterations:  217 train result :  0.7826 test result:  0.72 loss:  174.672 temps calcul:  0.7249\n",
      "Iterations:  218 train result :  0.7826 test result:  0.72 loss:  174.665 temps calcul:  0.7309\n",
      "Iterations:  219 train result :  0.7825 test result:  0.717 loss:  174.655 temps calcul:  0.7249\n",
      "Iterations:  220 train result :  0.7825 test result:  0.716 loss:  174.643 temps calcul:  0.7259\n",
      "Iterations:  221 train result :  0.7831 test result:  0.717 loss:  174.644 temps calcul:  0.7309\n",
      "Iterations:  222 train result :  0.7827 test result:  0.718 loss:  174.634 temps calcul:  0.7199\n",
      "Iterations:  223 train result :  0.7829 test result:  0.719 loss:  174.625 temps calcul:  0.7229\n",
      "Iterations:  224 train result :  0.7831 test result:  0.719 loss:  174.618 temps calcul:  0.7299\n",
      "Iterations:  225 train result :  0.7831 test result:  0.718 loss:  174.609 temps calcul:  0.735\n",
      "Iterations:  226 train result :  0.7833 test result:  0.717 loss:  174.605 temps calcul:  0.7199\n",
      "Iterations:  227 train result :  0.7834 test result:  0.716 loss:  174.594 temps calcul:  0.7239\n",
      "Iterations:  228 train result :  0.7833 test result:  0.716 loss:  174.59 temps calcul:  0.735\n",
      "Iterations:  229 train result :  0.7834 test result:  0.716 loss:  174.584 temps calcul:  0.7239\n",
      "Iterations:  230 train result :  0.7836 test result:  0.717 loss:  174.573 temps calcul:  0.7229\n",
      "Iterations:  231 train result :  0.7838 test result:  0.715 loss:  174.586 temps calcul:  0.7249\n",
      "Iterations:  232 train result :  0.7836 test result:  0.716 loss:  174.568 temps calcul:  0.7299\n",
      "Iterations:  233 train result :  0.7838 test result:  0.715 loss:  174.557 temps calcul:  0.7189\n",
      "Iterations:  234 train result :  0.7838 test result:  0.715 loss:  174.55 temps calcul:  0.7229\n",
      "Iterations:  235 train result :  0.7837 test result:  0.712 loss:  174.543 temps calcul:  0.7289\n",
      "Iterations:  236 train result :  0.7838 test result:  0.714 loss:  174.538 temps calcul:  0.7229\n",
      "Iterations:  237 train result :  0.7839 test result:  0.716 loss:  174.532 temps calcul:  0.7159\n",
      "Iterations:  238 train result :  0.7838 test result:  0.715 loss:  174.525 temps calcul:  0.7239\n",
      "Iterations:  239 train result :  0.7838 test result:  0.714 loss:  174.519 temps calcul:  0.734\n",
      "Iterations:  240 train result :  0.7838 test result:  0.715 loss:  174.515 temps calcul:  0.7259\n",
      "Iterations:  241 train result :  0.7838 test result:  0.718 loss:  174.507 temps calcul:  0.7249\n",
      "Iterations:  242 train result :  0.7837 test result:  0.715 loss:  174.506 temps calcul:  0.7289\n",
      "Iterations:  243 train result :  0.7838 test result:  0.715 loss:  174.499 temps calcul:  0.7239\n",
      "Iterations:  244 train result :  0.7838 test result:  0.716 loss:  174.496 temps calcul:  0.7229\n",
      "Iterations:  245 train result :  0.7836 test result:  0.715 loss:  174.492 temps calcul:  0.7299\n",
      "Iterations:  246 train result :  0.7838 test result:  0.715 loss:  174.487 temps calcul:  0.7229\n",
      "Iterations:  247 train result :  0.7834 test result:  0.717 loss:  174.482 temps calcul:  0.7249\n",
      "Iterations:  248 train result :  0.7838 test result:  0.716 loss:  174.478 temps calcul:  0.7259\n",
      "Iterations:  249 train result :  0.7838 test result:  0.717 loss:  174.475 temps calcul:  0.7279\n",
      "Iterations:  250 train result :  0.7837 test result:  0.714 loss:  174.471 temps calcul:  0.7209\n",
      "Iterations:  251 train result :  0.7839 test result:  0.716 loss:  174.474 temps calcul:  0.7289\n",
      "Iterations:  252 train result :  0.7838 test result:  0.716 loss:  174.468 temps calcul:  0.7279\n",
      "Iterations:  253 train result :  0.7838 test result:  0.715 loss:  174.462 temps calcul:  0.7219\n",
      "Iterations:  254 train result :  0.7839 test result:  0.713 loss:  174.457 temps calcul:  0.7239\n",
      "Iterations:  255 train result :  0.7839 test result:  0.714 loss:  174.452 temps calcul:  0.7269\n",
      "Iterations:  256 train result :  0.7842 test result:  0.713 loss:  174.449 temps calcul:  0.7309\n",
      "Iterations:  257 train result :  0.784 test result:  0.712 loss:  174.445 temps calcul:  0.7259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations:  258 train result :  0.7841 test result:  0.712 loss:  174.441 temps calcul:  0.7259\n",
      "Iterations:  259 train result :  0.784 test result:  0.712 loss:  174.438 temps calcul:  0.7209\n",
      "Iterations:  260 train result :  0.7843 test result:  0.713 loss:  174.434 temps calcul:  0.7169\n",
      "Iterations:  261 train result :  0.7841 test result:  0.714 loss:  174.429 temps calcul:  0.7209\n",
      "Iterations:  262 train result :  0.784 test result:  0.715 loss:  174.426 temps calcul:  0.746\n",
      "Iterations:  263 train result :  0.784 test result:  0.715 loss:  174.423 temps calcul:  0.734\n",
      "Iterations:  264 train result :  0.784 test result:  0.716 loss:  174.419 temps calcul:  0.8914\n",
      "Iterations:  265 train result :  0.7842 test result:  0.715 loss:  174.414 temps calcul:  0.7711\n",
      "Iterations:  266 train result :  0.7845 test result:  0.715 loss:  174.411 temps calcul:  0.8483\n",
      "Iterations:  267 train result :  0.7846 test result:  0.715 loss:  174.408 temps calcul:  0.8242\n",
      "Iterations:  268 train result :  0.7845 test result:  0.715 loss:  174.406 temps calcul:  0.7991\n",
      "Iterations:  269 train result :  0.7846 test result:  0.715 loss:  174.403 temps calcul:  0.8122\n",
      "Iterations:  270 train result :  0.7846 test result:  0.715 loss:  174.399 temps calcul:  0.9034\n",
      "Iterations:  271 train result :  0.7845 test result:  0.715 loss:  174.397 temps calcul:  0.8051\n",
      "Iterations:  272 train result :  0.7846 test result:  0.714 loss:  174.394 temps calcul:  0.737\n",
      "Iterations:  273 train result :  0.7846 test result:  0.714 loss:  174.392 temps calcul:  0.738\n",
      "Iterations:  274 train result :  0.7846 test result:  0.714 loss:  174.39 temps calcul:  0.7299\n",
      "Iterations:  275 train result :  0.7845 test result:  0.713 loss:  174.388 temps calcul:  0.745\n",
      "Iterations:  276 train result :  0.7845 test result:  0.712 loss:  174.385 temps calcul:  0.8362\n",
      "Iterations:  277 train result :  0.7844 test result:  0.712 loss:  174.383 temps calcul:  0.742\n",
      "Iterations:  278 train result :  0.7844 test result:  0.713 loss:  174.381 temps calcul:  0.7981\n",
      "Iterations:  279 train result :  0.7845 test result:  0.713 loss:  174.378 temps calcul:  0.7771\n",
      "Iterations:  280 train result :  0.7843 test result:  0.714 loss:  174.381 temps calcul:  0.8071\n",
      "Iterations:  281 train result :  0.7844 test result:  0.713 loss:  174.376 temps calcul:  0.7951\n",
      "Iterations:  282 train result :  0.7847 test result:  0.713 loss:  174.374 temps calcul:  0.8302\n",
      "Iterations:  283 train result :  0.7846 test result:  0.713 loss:  174.372 temps calcul:  0.8182\n",
      "Iterations:  284 train result :  0.7847 test result:  0.714 loss:  174.37 temps calcul:  0.76\n",
      "Iterations:  285 train result :  0.7848 test result:  0.714 loss:  174.369 temps calcul:  0.8202\n",
      "Iterations:  286 train result :  0.7845 test result:  0.714 loss:  174.366 temps calcul:  0.7891\n",
      "Iterations:  287 train result :  0.7844 test result:  0.714 loss:  174.365 temps calcul:  0.8212\n",
      "Iterations:  288 train result :  0.7846 test result:  0.714 loss:  174.364 temps calcul:  0.8803\n",
      "Iterations:  289 train result :  0.7846 test result:  0.714 loss:  174.361 temps calcul:  0.8583\n",
      "Iterations:  290 train result :  0.7847 test result:  0.715 loss:  174.366 temps calcul:  0.8643\n",
      "Iterations:  291 train result :  0.7847 test result:  0.714 loss:  174.36 temps calcul:  0.7841\n",
      "Iterations:  292 train result :  0.7847 test result:  0.714 loss:  174.357 temps calcul:  0.743\n",
      "Iterations:  293 train result :  0.7848 test result:  0.714 loss:  174.356 temps calcul:  0.7299\n",
      "Iterations:  294 train result :  0.785 test result:  0.714 loss:  174.353 temps calcul:  0.763\n",
      "Iterations:  295 train result :  0.785 test result:  0.714 loss:  174.352 temps calcul:  0.74\n",
      "Iterations:  296 train result :  0.7849 test result:  0.715 loss:  174.351 temps calcul:  0.8061\n",
      "Iterations:  297 train result :  0.7849 test result:  0.715 loss:  174.349 temps calcul:  0.8743\n",
      "Iterations:  298 train result :  0.7849 test result:  0.715 loss:  174.348 temps calcul:  0.746\n",
      "Iterations:  299 train result :  0.785 test result:  0.715 loss:  174.346 temps calcul:  0.746\n",
      "Iterations:  300 train result :  0.7847 test result:  0.715 loss:  174.344 temps calcul:  0.7891\n",
      "Iterations:  301 train result :  0.7848 test result:  0.715 loss:  174.343 temps calcul:  0.758\n",
      "Iterations:  302 train result :  0.785 test result:  0.714 loss:  174.342 temps calcul:  0.7329\n",
      "Iterations:  303 train result :  0.7851 test result:  0.714 loss:  174.341 temps calcul:  0.734\n",
      "Iterations:  304 train result :  0.785 test result:  0.715 loss:  174.339 temps calcul:  0.7259\n",
      "Iterations:  305 train result :  0.7851 test result:  0.715 loss:  174.338 temps calcul:  0.7219\n",
      "Iterations:  306 train result :  0.7851 test result:  0.715 loss:  174.337 temps calcul:  0.738\n",
      "Iterations:  307 train result :  0.785 test result:  0.715 loss:  174.336 temps calcul:  0.7229\n",
      "Iterations:  308 train result :  0.785 test result:  0.715 loss:  174.335 temps calcul:  0.7219\n",
      "Iterations:  309 train result :  0.7851 test result:  0.715 loss:  174.334 temps calcul:  0.7319\n",
      "Iterations:  310 train result :  0.7851 test result:  0.715 loss:  174.333 temps calcul:  0.7259\n",
      "Iterations:  311 train result :  0.7852 test result:  0.715 loss:  174.332 temps calcul:  0.7239\n",
      "Iterations:  312 train result :  0.7851 test result:  0.715 loss:  174.331 temps calcul:  0.7209\n",
      "Iterations:  313 train result :  0.785 test result:  0.715 loss:  174.33 temps calcul:  0.7279\n",
      "Iterations:  314 train result :  0.7852 test result:  0.716 loss:  174.329 temps calcul:  0.7219\n",
      "Iterations:  315 train result :  0.7852 test result:  0.716 loss:  174.329 temps calcul:  0.7179\n",
      "Iterations:  316 train result :  0.7854 test result:  0.716 loss:  174.328 temps calcul:  0.7249\n",
      "Iterations:  317 train result :  0.7854 test result:  0.716 loss:  174.327 temps calcul:  0.7219\n",
      "Iterations:  318 train result :  0.7853 test result:  0.715 loss:  174.327 temps calcul:  0.7249\n",
      "Iterations:  319 train result :  0.7854 test result:  0.715 loss:  174.325 temps calcul:  0.7209\n",
      "Iterations:  320 train result :  0.7851 test result:  0.715 loss:  174.327 temps calcul:  0.7289\n",
      "Iterations:  321 train result :  0.7853 test result:  0.715 loss:  174.325 temps calcul:  0.746\n",
      "Iterations:  322 train result :  0.7853 test result:  0.715 loss:  174.324 temps calcul:  0.8242\n",
      "Iterations:  323 train result :  0.7853 test result:  0.715 loss:  174.323 temps calcul:  0.8242\n",
      "Iterations:  324 train result :  0.7854 test result:  0.714 loss:  174.322 temps calcul:  0.8262\n",
      "Iterations:  325 train result :  0.7854 test result:  0.714 loss:  174.321 temps calcul:  0.8262\n",
      "Iterations:  326 train result :  0.7853 test result:  0.714 loss:  174.321 temps calcul:  0.8643\n",
      "Iterations:  327 train result :  0.7852 test result:  0.713 loss:  174.32 temps calcul:  0.744\n",
      "Iterations:  328 train result :  0.7852 test result:  0.713 loss:  174.319 temps calcul:  0.74\n",
      "Iterations:  329 train result :  0.7853 test result:  0.713 loss:  174.319 temps calcul:  0.739\n",
      "Iterations:  330 train result :  0.7853 test result:  0.713 loss:  174.318 temps calcul:  0.733\n",
      "Iterations:  331 train result :  0.7854 test result:  0.714 loss:  174.317 temps calcul:  0.744\n",
      "Iterations:  332 train result :  0.7854 test result:  0.714 loss:  174.317 temps calcul:  0.8924\n",
      "Iterations:  333 train result :  0.7853 test result:  0.714 loss:  174.316 temps calcul:  0.749\n",
      "Iterations:  334 train result :  0.7853 test result:  0.714 loss:  174.315 temps calcul:  0.735\n",
      "Iterations:  335 train result :  0.7856 test result:  0.714 loss:  174.315 temps calcul:  0.7319\n",
      "Iterations:  336 train result :  0.7855 test result:  0.714 loss:  174.314 temps calcul:  0.7209\n",
      "Iterations:  337 train result :  0.7855 test result:  0.714 loss:  174.314 temps calcul:  0.7179\n",
      "Iterations:  338 train result :  0.7855 test result:  0.714 loss:  174.313 temps calcul:  0.7229\n",
      "Iterations:  339 train result :  0.7857 test result:  0.714 loss:  174.313 temps calcul:  0.7299\n",
      "Iterations:  340 train result :  0.7856 test result:  0.714 loss:  174.313 temps calcul:  0.7279\n",
      "Iterations:  341 train result :  0.7856 test result:  0.714 loss:  174.312 temps calcul:  0.7199\n",
      "Iterations:  342 train result :  0.7856 test result:  0.714 loss:  174.311 temps calcul:  0.7269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations:  343 train result :  0.7853 test result:  0.715 loss:  174.311 temps calcul:  0.7249\n",
      "Iterations:  344 train result :  0.7856 test result:  0.715 loss:  174.31 temps calcul:  0.7179\n",
      "Iterations:  345 train result :  0.7855 test result:  0.715 loss:  174.31 temps calcul:  0.7199\n",
      "Iterations:  346 train result :  0.7854 test result:  0.715 loss:  174.309 temps calcul:  0.7239\n",
      "Iterations:  347 train result :  0.7855 test result:  0.715 loss:  174.309 temps calcul:  0.733\n",
      "Iterations:  348 train result :  0.7855 test result:  0.715 loss:  174.309 temps calcul:  0.7179\n",
      "Iterations:  349 train result :  0.7856 test result:  0.715 loss:  174.308 temps calcul:  0.7239\n",
      "Iterations:  350 train result :  0.7855 test result:  0.715 loss:  174.308 temps calcul:  0.7319\n",
      "Iterations:  351 train result :  0.7855 test result:  0.715 loss:  174.307 temps calcul:  0.7209\n",
      "Iterations:  352 train result :  0.7855 test result:  0.715 loss:  174.307 temps calcul:  0.7199\n",
      "Iterations:  353 train result :  0.7856 test result:  0.715 loss:  174.307 temps calcul:  0.7249\n",
      "Iterations:  354 train result :  0.7855 test result:  0.715 loss:  174.306 temps calcul:  0.736\n",
      "Iterations:  355 train result :  0.7856 test result:  0.715 loss:  174.306 temps calcul:  0.7249\n",
      "Iterations:  356 train result :  0.7854 test result:  0.714 loss:  174.306 temps calcul:  0.7219\n",
      "Iterations:  357 train result :  0.7854 test result:  0.714 loss:  174.305 temps calcul:  0.733\n",
      "Iterations:  358 train result :  0.7856 test result:  0.715 loss:  174.305 temps calcul:  0.7219\n",
      "Iterations:  359 train result :  0.7856 test result:  0.715 loss:  174.305 temps calcul:  0.7219\n",
      "Iterations:  360 train result :  0.7855 test result:  0.715 loss:  174.304 temps calcul:  0.7319\n",
      "Iterations:  361 train result :  0.7856 test result:  0.714 loss:  174.304 temps calcul:  0.7209\n",
      "Iterations:  362 train result :  0.7855 test result:  0.714 loss:  174.304 temps calcul:  0.7199\n",
      "Iterations:  363 train result :  0.7855 test result:  0.714 loss:  174.303 temps calcul:  0.7239\n",
      "Iterations:  364 train result :  0.7855 test result:  0.714 loss:  174.303 temps calcul:  0.7279\n",
      "Iterations:  365 train result :  0.7854 test result:  0.715 loss:  174.303 temps calcul:  0.7249\n",
      "Iterations:  366 train result :  0.7854 test result:  0.714 loss:  174.302 temps calcul:  0.7249\n",
      "Iterations:  367 train result :  0.7854 test result:  0.714 loss:  174.302 temps calcul:  0.7229\n",
      "Iterations:  368 train result :  0.7853 test result:  0.714 loss:  174.302 temps calcul:  0.7179\n",
      "Iterations:  369 train result :  0.7854 test result:  0.715 loss:  174.302 temps calcul:  0.7229\n",
      "Iterations:  370 train result :  0.7855 test result:  0.714 loss:  174.302 temps calcul:  0.7239\n",
      "Iterations:  371 train result :  0.7855 test result:  0.714 loss:  174.301 temps calcul:  0.7269\n",
      "Iterations:  372 train result :  0.7855 test result:  0.715 loss:  174.301 temps calcul:  0.7209\n",
      "Iterations:  373 train result :  0.7855 test result:  0.715 loss:  174.301 temps calcul:  0.7269\n",
      "Iterations:  374 train result :  0.7854 test result:  0.715 loss:  174.301 temps calcul:  0.7229\n",
      "Iterations:  375 train result :  0.7854 test result:  0.714 loss:  174.3 temps calcul:  0.7219\n",
      "Iterations:  376 train result :  0.7853 test result:  0.714 loss:  174.3 temps calcul:  0.7259\n",
      "Iterations:  377 train result :  0.7853 test result:  0.714 loss:  174.3 temps calcul:  0.7751\n",
      "Iterations:  378 train result :  0.7853 test result:  0.714 loss:  174.3 temps calcul:  0.8382\n",
      "Iterations:  379 train result :  0.7854 test result:  0.714 loss:  174.299 temps calcul:  0.8232\n",
      "Iterations:  380 train result :  0.7854 test result:  0.714 loss:  174.299 temps calcul:  0.8322\n",
      "Iterations:  381 train result :  0.7856 test result:  0.714 loss:  174.299 temps calcul:  0.8503\n",
      "Iterations:  382 train result :  0.7856 test result:  0.715 loss:  174.299 temps calcul:  0.8092\n",
      "Iterations:  383 train result :  0.7855 test result:  0.715 loss:  174.299 temps calcul:  0.751\n",
      "Iterations:  384 train result :  0.7856 test result:  0.715 loss:  174.298 temps calcul:  0.8733\n",
      "Iterations:  385 train result :  0.7855 test result:  0.714 loss:  174.298 temps calcul:  0.7269\n",
      "Iterations:  386 train result :  0.7855 test result:  0.714 loss:  174.298 temps calcul:  0.7269\n",
      "Iterations:  387 train result :  0.7855 test result:  0.714 loss:  174.298 temps calcul:  0.7319\n",
      "Iterations:  388 train result :  0.7856 test result:  0.714 loss:  174.298 temps calcul:  0.7249\n",
      "Iterations:  389 train result :  0.7857 test result:  0.714 loss:  174.298 temps calcul:  0.7259\n",
      "Iterations:  390 train result :  0.7858 test result:  0.715 loss:  174.298 temps calcul:  0.7299\n",
      "Iterations:  391 train result :  0.7857 test result:  0.715 loss:  174.297 temps calcul:  0.7229\n",
      "Iterations:  392 train result :  0.7857 test result:  0.715 loss:  174.297 temps calcul:  0.7209\n",
      "Iterations:  393 train result :  0.7856 test result:  0.715 loss:  174.297 temps calcul:  0.7249\n",
      "Iterations:  394 train result :  0.7856 test result:  0.715 loss:  174.297 temps calcul:  0.7269\n",
      "Iterations:  395 train result :  0.7855 test result:  0.715 loss:  174.297 temps calcul:  0.747\n",
      "Iterations:  396 train result :  0.7855 test result:  0.715 loss:  174.297 temps calcul:  0.7871\n",
      "Iterations:  397 train result :  0.7855 test result:  0.715 loss:  174.297 temps calcul:  0.8222\n",
      "Iterations:  398 train result :  0.7856 test result:  0.715 loss:  174.297 temps calcul:  0.7821\n",
      "Iterations:  399 train result :  0.7854 test result:  0.715 loss:  174.296 temps calcul:  0.756\n",
      "Iterations:  400 train result :  0.7853 test result:  0.715 loss:  174.296 temps calcul:  0.8222\n",
      "Iterations:  401 train result :  0.7855 test result:  0.715 loss:  174.296 temps calcul:  0.745\n",
      "Iterations:  402 train result :  0.7855 test result:  0.714 loss:  174.296 temps calcul:  0.8272\n",
      "Iterations:  403 train result :  0.7856 test result:  0.715 loss:  174.296 temps calcul:  0.752\n",
      "Iterations:  404 train result :  0.7856 test result:  0.715 loss:  174.296 temps calcul:  0.7309\n",
      "Iterations:  405 train result :  0.7857 test result:  0.715 loss:  174.296 temps calcul:  0.7279\n",
      "Iterations:  406 train result :  0.7857 test result:  0.715 loss:  174.296 temps calcul:  0.7329\n",
      "Iterations:  407 train result :  0.7857 test result:  0.714 loss:  174.296 temps calcul:  0.7179\n",
      "Iterations:  408 train result :  0.7857 test result:  0.714 loss:  174.296 temps calcul:  0.7239\n",
      "Iterations:  409 train result :  0.7857 test result:  0.714 loss:  174.295 temps calcul:  0.7289\n",
      "Iterations:  410 train result :  0.7856 test result:  0.714 loss:  174.295 temps calcul:  0.7209\n",
      "Iterations:  411 train result :  0.7857 test result:  0.714 loss:  174.295 temps calcul:  0.7239\n",
      "Iterations:  412 train result :  0.7857 test result:  0.714 loss:  174.295 temps calcul:  0.7239\n",
      "Iterations:  413 train result :  0.7856 test result:  0.714 loss:  174.295 temps calcul:  0.733\n",
      "Iterations:  414 train result :  0.7856 test result:  0.715 loss:  174.295 temps calcul:  0.7239\n",
      "Iterations:  415 train result :  0.7857 test result:  0.715 loss:  174.295 temps calcul:  0.7259\n",
      "Iterations:  416 train result :  0.7857 test result:  0.715 loss:  174.295 temps calcul:  0.7199\n",
      "Iterations:  417 train result :  0.7857 test result:  0.715 loss:  174.295 temps calcul:  0.7249\n",
      "Iterations:  418 train result :  0.7858 test result:  0.715 loss:  174.295 temps calcul:  0.7229\n",
      "Iterations:  419 train result :  0.7857 test result:  0.715 loss:  174.294 temps calcul:  0.7239\n",
      "Iterations:  420 train result :  0.7857 test result:  0.716 loss:  174.294 temps calcul:  0.7309\n",
      "Iterations:  421 train result :  0.7857 test result:  0.716 loss:  174.294 temps calcul:  0.7249\n",
      "Iterations:  422 train result :  0.7857 test result:  0.715 loss:  174.294 temps calcul:  0.7269\n",
      "Iterations:  423 train result :  0.7857 test result:  0.715 loss:  174.294 temps calcul:  0.7309\n",
      "Iterations:  424 train result :  0.7859 test result:  0.715 loss:  174.294 temps calcul:  0.7204\n",
      "Iterations:  425 train result :  0.7858 test result:  0.715 loss:  174.294 temps calcul:  0.7289\n",
      "Iterations:  426 train result :  0.7858 test result:  0.715 loss:  174.294 temps calcul:  0.746\n",
      "Iterations:  427 train result :  0.7858 test result:  0.714 loss:  174.294 temps calcul:  0.8944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations:  428 train result :  0.7858 test result:  0.715 loss:  174.294 temps calcul:  0.8132\n",
      "Iterations:  429 train result :  0.7859 test result:  0.713 loss:  174.294 temps calcul:  0.759\n",
      "Iterations:  430 train result :  0.7859 test result:  0.713 loss:  174.294 temps calcul:  0.7269\n",
      "Iterations:  431 train result :  0.7858 test result:  0.713 loss:  174.294 temps calcul:  0.7269\n",
      "Iterations:  432 train result :  0.786 test result:  0.713 loss:  174.294 temps calcul:  0.7239\n",
      "Iterations:  433 train result :  0.786 test result:  0.713 loss:  174.293 temps calcul:  0.7259\n",
      "Iterations:  434 train result :  0.7859 test result:  0.713 loss:  174.293 temps calcul:  0.7921\n",
      "Iterations:  435 train result :  0.786 test result:  0.713 loss:  174.293 temps calcul:  0.8172\n",
      "Iterations:  436 train result :  0.7859 test result:  0.713 loss:  174.293 temps calcul:  0.8483\n",
      "Iterations:  437 train result :  0.7859 test result:  0.714 loss:  174.293 temps calcul:  0.8051\n",
      "Iterations:  438 train result :  0.786 test result:  0.714 loss:  174.293 temps calcul:  0.766\n",
      "Iterations:  439 train result :  0.7859 test result:  0.714 loss:  174.293 temps calcul:  0.74\n",
      "Iterations:  440 train result :  0.7859 test result:  0.714 loss:  174.293 temps calcul:  0.742\n",
      "Iterations:  441 train result :  0.7859 test result:  0.714 loss:  174.293 temps calcul:  0.8442\n",
      "Iterations:  442 train result :  0.7859 test result:  0.714 loss:  174.293 temps calcul:  0.74\n",
      "Iterations:  443 train result :  0.7859 test result:  0.713 loss:  174.293 temps calcul:  0.8232\n",
      "Iterations:  444 train result :  0.786 test result:  0.713 loss:  174.293 temps calcul:  0.7711\n",
      "Iterations:  445 train result :  0.7859 test result:  0.713 loss:  174.293 temps calcul:  0.739\n",
      "Iterations:  446 train result :  0.7859 test result:  0.713 loss:  174.293 temps calcul:  0.7239\n",
      "Iterations:  447 train result :  0.786 test result:  0.713 loss:  174.293 temps calcul:  0.7289\n",
      "Iterations:  448 train result :  0.786 test result:  0.713 loss:  174.293 temps calcul:  0.7239\n",
      "Iterations:  449 train result :  0.786 test result:  0.714 loss:  174.293 temps calcul:  0.7309\n",
      "Iterations:  450 train result :  0.7861 test result:  0.714 loss:  174.293 temps calcul:  0.7289\n",
      "Iterations:  451 train result :  0.7861 test result:  0.714 loss:  174.293 temps calcul:  0.7189\n",
      "Iterations:  452 train result :  0.786 test result:  0.714 loss:  174.293 temps calcul:  0.7199\n",
      "Iterations:  453 train result :  0.7861 test result:  0.714 loss:  174.293 temps calcul:  0.7229\n",
      "Iterations:  454 train result :  0.786 test result:  0.714 loss:  174.292 temps calcul:  0.7259\n",
      "Iterations:  455 train result :  0.786 test result:  0.714 loss:  174.292 temps calcul:  0.7189\n",
      "Iterations:  456 train result :  0.7859 test result:  0.714 loss:  174.292 temps calcul:  0.7229\n",
      "Iterations:  457 train result :  0.7859 test result:  0.714 loss:  174.292 temps calcul:  0.7269\n",
      "Iterations:  458 train result :  0.7859 test result:  0.714 loss:  174.292 temps calcul:  0.7199\n",
      "Iterations:  459 train result :  0.786 test result:  0.714 loss:  174.292 temps calcul:  0.7189\n",
      "Iterations:  460 train result :  0.786 test result:  0.714 loss:  174.292 temps calcul:  0.742\n",
      "Iterations:  461 train result :  0.7861 test result:  0.714 loss:  174.292 temps calcul:  0.7289\n",
      "Iterations:  462 train result :  0.7859 test result:  0.714 loss:  174.292 temps calcul:  0.7199\n",
      "Iterations:  463 train result :  0.786 test result:  0.714 loss:  174.292 temps calcul:  0.749\n",
      "Iterations:  464 train result :  0.7859 test result:  0.714 loss:  174.292 temps calcul:  0.8202\n",
      "Iterations:  465 train result :  0.7859 test result:  0.714 loss:  174.292 temps calcul:  0.8172\n",
      "Iterations:  466 train result :  0.7859 test result:  0.714 loss:  174.292 temps calcul:  0.749\n",
      "Iterations:  467 train result :  0.7858 test result:  0.714 loss:  174.292 temps calcul:  0.7309\n",
      "Iterations:  468 train result :  0.7859 test result:  0.714 loss:  174.292 temps calcul:  0.738\n",
      "Iterations:  469 train result :  0.7859 test result:  0.714 loss:  174.292 temps calcul:  0.747\n",
      "Iterations:  470 train result :  0.7858 test result:  0.714 loss:  174.292 temps calcul:  0.8172\n",
      "Iterations:  471 train result :  0.7858 test result:  0.714 loss:  174.292 temps calcul:  0.755\n",
      "Iterations:  472 train result :  0.7858 test result:  0.714 loss:  174.292 temps calcul:  0.746\n",
      "Iterations:  473 train result :  0.7858 test result:  0.714 loss:  174.292 temps calcul:  0.8854\n",
      "Iterations:  474 train result :  0.7858 test result:  0.714 loss:  174.292 temps calcul:  0.746\n",
      "Iterations:  475 train result :  0.7859 test result:  0.714 loss:  174.292 temps calcul:  0.9004\n",
      "Iterations:  476 train result :  0.7859 test result:  0.714 loss:  174.292 temps calcul:  0.737\n",
      "Iterations:  477 train result :  0.7859 test result:  0.714 loss:  174.292 temps calcul:  0.9114\n",
      "Iterations:  478 train result :  0.7859 test result:  0.714 loss:  174.292 temps calcul:  0.8112\n",
      "Iterations:  479 train result :  0.7859 test result:  0.714 loss:  174.292 temps calcul:  0.7299\n",
      "Iterations:  480 train result :  0.7859 test result:  0.714 loss:  174.292 temps calcul:  0.7219\n",
      "Iterations:  481 train result :  0.7859 test result:  0.714 loss:  174.292 temps calcul:  0.733\n",
      "Iterations:  482 train result :  0.7859 test result:  0.714 loss:  174.292 temps calcul:  0.7269\n",
      "Iterations:  483 train result :  0.7859 test result:  0.714 loss:  174.292 temps calcul:  0.7279\n",
      "Iterations:  484 train result :  0.786 test result:  0.714 loss:  174.292 temps calcul:  0.735\n",
      "Iterations:  485 train result :  0.786 test result:  0.714 loss:  174.292 temps calcul:  0.7219\n",
      "Iterations:  486 train result :  0.786 test result:  0.714 loss:  174.292 temps calcul:  0.7229\n",
      "Iterations:  487 train result :  0.7859 test result:  0.714 loss:  174.292 temps calcul:  0.7279\n",
      "Iterations:  488 train result :  0.7859 test result:  0.714 loss:  174.292 temps calcul:  0.7289\n",
      "Iterations:  489 train result :  0.7859 test result:  0.714 loss:  174.292 temps calcul:  0.741\n",
      "Iterations:  490 train result :  0.7859 test result:  0.714 loss:  174.292 temps calcul:  0.749\n",
      "Iterations:  491 train result :  0.786 test result:  0.714 loss:  174.292 temps calcul:  0.741\n",
      "Iterations:  492 train result :  0.786 test result:  0.714 loss:  174.292 temps calcul:  0.733\n",
      "Iterations:  493 train result :  0.786 test result:  0.714 loss:  174.292 temps calcul:  0.746\n",
      "Iterations:  494 train result :  0.786 test result:  0.715 loss:  174.292 temps calcul:  0.737\n",
      "Iterations:  495 train result :  0.786 test result:  0.715 loss:  174.292 temps calcul:  0.737\n",
      "Iterations:  496 train result :  0.786 test result:  0.715 loss:  174.292 temps calcul:  0.7299\n",
      "Iterations:  497 train result :  0.7859 test result:  0.715 loss:  174.291 temps calcul:  0.7259\n",
      "Iterations:  498 train result :  0.786 test result:  0.715 loss:  174.291 temps calcul:  0.748\n",
      "Iterations:  499 train result :  0.7859 test result:  0.715 loss:  174.291 temps calcul:  0.8182\n",
      "Iterations:  500 train result :  0.7859 test result:  0.715 loss:  174.291 temps calcul:  0.751\n",
      "Iterations:  501 train result :  0.786 test result:  0.715 loss:  174.291 temps calcul:  0.8793\n",
      "Iterations:  502 train result :  0.7859 test result:  0.715 loss:  174.291 temps calcul:  0.752\n",
      "Iterations:  503 train result :  0.786 test result:  0.714 loss:  174.291 temps calcul:  0.8553\n",
      "Iterations:  504 train result :  0.786 test result:  0.714 loss:  174.291 temps calcul:  0.744\n",
      "Iterations:  505 train result :  0.786 test result:  0.714 loss:  174.291 temps calcul:  0.7299\n",
      "Iterations:  506 train result :  0.786 test result:  0.714 loss:  174.291 temps calcul:  0.743\n",
      "Iterations:  507 train result :  0.786 test result:  0.714 loss:  174.291 temps calcul:  0.7761\n",
      "Iterations:  508 train result :  0.786 test result:  0.714 loss:  174.291 temps calcul:  0.75\n",
      "Iterations:  509 train result :  0.786 test result:  0.714 loss:  174.291 temps calcul:  0.755\n",
      "Iterations:  510 train result :  0.786 test result:  0.714 loss:  174.291 temps calcul:  0.752\n",
      "Iterations:  511 train result :  0.786 test result:  0.715 loss:  174.291 temps calcul:  0.7721\n",
      "Iterations:  512 train result :  0.786 test result:  0.714 loss:  174.291 temps calcul:  0.736\n",
      "Iterations:  513 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations:  514 train result :  0.786 test result:  0.714 loss:  174.291 temps calcul:  0.741\n",
      "Iterations:  515 train result :  0.786 test result:  0.714 loss:  174.291 temps calcul:  0.7721\n",
      "Iterations:  516 train result :  0.786 test result:  0.714 loss:  174.291 temps calcul:  0.7269\n",
      "Iterations:  517 train result :  0.786 test result:  0.714 loss:  174.291 temps calcul:  0.7309\n",
      "Iterations:  518 train result :  0.786 test result:  0.714 loss:  174.291 temps calcul:  0.752\n",
      "Iterations:  519 train result :  0.786 test result:  0.714 loss:  174.291 temps calcul:  0.739\n",
      "Iterations:  520 train result :  0.786 test result:  0.714 loss:  174.291 temps calcul:  0.735\n",
      "Iterations:  521 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.754\n",
      "Iterations:  522 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.8011\n",
      "Iterations:  523 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.8152\n",
      "Iterations:  524 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.75\n",
      "Iterations:  525 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.741\n",
      "Iterations:  526 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.7279\n",
      "Iterations:  527 train result :  0.786 test result:  0.714 loss:  174.291 temps calcul:  0.7199\n",
      "Iterations:  528 train result :  0.786 test result:  0.714 loss:  174.291 temps calcul:  0.741\n",
      "Iterations:  529 train result :  0.786 test result:  0.714 loss:  174.291 temps calcul:  0.7911\n",
      "Iterations:  530 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.7771\n",
      "Iterations:  531 train result :  0.786 test result:  0.714 loss:  174.291 temps calcul:  0.7801\n",
      "Iterations:  532 train result :  0.786 test result:  0.714 loss:  174.291 temps calcul:  0.748\n",
      "Iterations:  533 train result :  0.786 test result:  0.714 loss:  174.291 temps calcul:  0.746\n",
      "Iterations:  534 train result :  0.786 test result:  0.714 loss:  174.291 temps calcul:  0.7741\n",
      "Iterations:  535 train result :  0.786 test result:  0.714 loss:  174.291 temps calcul:  0.744\n",
      "Iterations:  536 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.7931\n",
      "Iterations:  537 train result :  0.786 test result:  0.714 loss:  174.291 temps calcul:  0.7771\n",
      "Iterations:  538 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.7901\n",
      "Iterations:  539 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.8222\n",
      "Iterations:  540 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.8001\n",
      "Iterations:  541 train result :  0.786 test result:  0.714 loss:  174.291 temps calcul:  0.8713\n",
      "Iterations:  542 train result :  0.786 test result:  0.714 loss:  174.291 temps calcul:  0.8964\n",
      "Iterations:  543 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.8422\n",
      "Iterations:  544 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.747\n",
      "Iterations:  545 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.7239\n",
      "Iterations:  546 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.7289\n",
      "Iterations:  547 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.7289\n",
      "Iterations:  548 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.7219\n",
      "Iterations:  549 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.7199\n",
      "Iterations:  550 train result :  0.786 test result:  0.714 loss:  174.291 temps calcul:  0.7229\n",
      "Iterations:  551 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.7249\n",
      "Iterations:  552 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.748\n",
      "Iterations:  553 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.746\n",
      "Iterations:  554 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.76\n",
      "Iterations:  555 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.7741\n",
      "Iterations:  556 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.762\n",
      "Iterations:  557 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.734\n",
      "Iterations:  558 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.746\n",
      "Iterations:  559 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.7319\n",
      "Iterations:  560 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.753\n",
      "Iterations:  561 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.8382\n",
      "Iterations:  562 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.746\n",
      "Iterations:  563 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.7299\n",
      "Iterations:  564 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.7259\n",
      "Iterations:  565 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.7329\n",
      "Iterations:  566 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.7309\n",
      "Iterations:  567 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.7239\n",
      "Iterations:  568 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.7229\n",
      "Iterations:  569 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.745\n",
      "Iterations:  570 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.7921\n",
      "Iterations:  571 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.8513\n",
      "Iterations:  572 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.7981\n",
      "Iterations:  573 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.8372\n",
      "Iterations:  574 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.753\n",
      "Iterations:  575 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.8885\n",
      "Iterations:  576 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.8192\n",
      "Iterations:  577 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.8041\n",
      "Iterations:  578 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.76\n",
      "Iterations:  579 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.751\n",
      "Iterations:  580 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.7721\n",
      "Iterations:  581 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.7299\n",
      "Iterations:  582 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.7761\n",
      "Iterations:  583 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.7741\n",
      "Iterations:  584 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.7269\n",
      "Iterations:  585 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.737\n",
      "Iterations:  586 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.745\n",
      "Iterations:  587 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.8683\n",
      "Iterations:  588 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.8142\n",
      "Iterations:  589 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.7731\n",
      "Iterations:  590 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.8252\n",
      "Iterations:  591 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.747\n",
      "Iterations:  592 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.8643\n",
      "Iterations:  593 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.753\n",
      "Iterations:  594 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.7289\n",
      "Iterations:  595 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.7941\n",
      "Iterations:  596 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.8683\n",
      "Iterations:  597 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.7711\n",
      "Iterations:  598 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations:  599 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.7329\n",
      "Iterations:  600 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.7821\n",
      "Iterations:  601 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.746\n",
      "Iterations:  602 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.7731\n",
      "Iterations:  603 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.748\n",
      "Iterations:  604 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.7319\n",
      "Iterations:  605 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.7289\n",
      "Iterations:  606 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.8011\n",
      "Iterations:  607 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.8061\n",
      "Iterations:  608 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.736\n",
      "Iterations:  609 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.768\n",
      "Iterations:  610 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.7741\n",
      "Iterations:  611 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.7299\n",
      "Iterations:  612 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.7239\n",
      "Iterations:  613 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.7229\n",
      "Iterations:  614 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.741\n",
      "Iterations:  615 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.8102\n",
      "Iterations:  616 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.743\n",
      "Iterations:  617 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.74\n",
      "Iterations:  618 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.746\n",
      "Iterations:  619 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.734\n",
      "Iterations:  620 train result :  0.7859 test result:  0.714 loss:  174.291 temps calcul:  0.7259\n",
      "nombre total d iterations 596\n"
     ]
    }
   ],
   "source": [
    "#from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "#Asc = preprocessing.scale(A)\n",
    "\n",
    "#Standardize the data \n",
    "Asc= A- np.mean(A,axis=0)\n",
    "Asc= Asc/np.sqrt(np.var(A,axis=0)+0.01)\n",
    "One= np.array([1]*49000)\n",
    "\n",
    "Asc=np.c_[ Asc, np.ones(len(A)) ] \n",
    "#Standardize the data of X_test with the data X_train\n",
    "test=extract_features (X_test )\n",
    "test=test- np.mean(A,axis=0)\n",
    "test= test/ np.sqrt(np.var(A,axis=0)+0.01)\n",
    "Tsc=np.c_[test, np.ones(len(test)) ] \n",
    "\n",
    "\n",
    "#train SVM with with the features of A (features from Xtrain) standardised , and Y_train=Cifar[1]+1\n",
    "\n",
    "theta=train_svm(Asc, y_train+1 , 100, Tsc, y_test)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib\n",
    "plt.plot(tabLoss)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib\n",
    "plt.plot(tabAccT)\n",
    "plt.plot(tabAccTest)\n",
    "plt.title('model accuracy 600 features SVM')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('iteration')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train result :\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.51587755102040811"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we find with f_min the function who find w  who minimize the fonction svmloss\n",
    "\n",
    "Matrice = Asc.dot(theta)      #theta= w  theta shape : (4* num Centroids, number of class ), Asc= x ,\n",
    "#Asc.shape = 50000, 4*numCentroids, Matrice (50000, 10) argamx axis= 1 sort the number of the class predict \n",
    "#higher score with weight vector \n",
    "[val1 ,inds1] = [np.max(Matrice, axis=1),np.argmax(Matrice, axis=1) ]  #y_pred= argmax(x*wy)\n",
    "print('train result :')\n",
    "(inds1==y_train).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Standardize the data of X_test with the data X_train\n",
    "reserve=BBB\n",
    "BBB=BBB- np.mean(A,axis=0)\n",
    "BBB= BBB/ np.sqrt(np.var(A,axis=0)+0.01)\n",
    "Bsc=np.c_[ BBB, np.ones(len(BBB)) ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuraccy result on Xval:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.54800000000000004"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MatriceTest= Bsc.dot(theta)\n",
    "[val2 ,inds2] = [np.max(MatriceTest, axis=1),np.argmax(MatriceTest, axis=1) ]\n",
    "print('Accuraccy result on Xval:')\n",
    "(inds2==y_val).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction features: \n",
      "Accuraccy result on Xtest:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.502"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=extract_features (X_test )\n",
    "test=test- np.mean(A,axis=0)\n",
    "test= test/ np.sqrt(np.var(A,axis=0)+0.01)\n",
    "Tsc=np.c_[test, np.ones(len(test)) ] \n",
    "MatriceTest1= Tsc.dot(theta)\n",
    "[val3 ,inds3] = [np.max(MatriceTest1, axis=1),np.argmax(MatriceTest1, axis=1) ]\n",
    "print('Accuraccy result on Xtest:')\n",
    "(inds3==y_test).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: TkAgg\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib\n",
    "plt.plot(tabLoss)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictio=train_svm(Asc, y_train+1 , 100)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time= time.time()\n",
    "\n",
    "a = start_time-time.time()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "%matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_test = y_test\n",
    "y_pred = inds3\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=['airplane', 'automobile', 'bird', 'cat', 'deer' ,'dog', 'frog', 'horse' ,'ship' ,'truck'],\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=['airplane', 'automobile', 'bird', 'cat', 'deer' ,'dog', 'frog', 'horse' ,'ship' ,'truck'], normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_size = A.shape[1]\n",
    "hidden_size = 200\n",
    "num_classes = 10\n",
    "\n",
    "Asc1= A- np.mean(A,axis=0)\n",
    "Asc1= Asc1/np.sqrt(np.var(A,axis=0)+0.01)\n",
    "net = TwoLayerNet(input_size, hidden_size, num_classes,1e-4)\n",
    "stats = net.train(Asc1, Cifar[1] , BBB, Cifar[3] ,\n",
    "                            num_iters=70000, batch_size=128,\n",
    "                            learning_rate=5e-4, learning_rate_decay=0.99,\n",
    "                            reg=0, verbose=True,update=\"SGD\",arg=0.95,dropout=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_acc = (net.predict(Asc1) == Cifar[1]).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "288.94487380762916"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tabLoss[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout , Activation\n",
    "from keras.optimizers import RMSprop\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import sklearn.preprocessing\n",
    "from keras.optimizers import SGD\n",
    "#optimizer=RMSprop(lr=0.0001, rho=0.9, epsilon = 1e-06, decay=1e-6)\n",
    "\n",
    "a=y_train\n",
    "label_binarizer = sklearn.preprocessing.LabelBinarizer()\n",
    "label_binarizer.fit(range(max(a)+1))\n",
    "C1=label_binarizer.transform(a)\n",
    "b=y_val\n",
    "C2=label_binarizer.transform(b)\n",
    "model= Sequential()\n",
    "model.add(Dense(50,input_shape=(A.shape[1],)))\n",
    "model.add(Dense(200))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "#sgd = SGD(lr=0.0001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr=0.0001, rho=0.9, epsilon = 1e-06, decay=1e-6), metrics=['accuracy'])\n",
    "history = model.fit(A, C1 , batch_size=1000, epochs=1000, verbose=2, validation_split=0.2)\n",
    "loss_and_metrics= model.evaluate(BBB, C2)\n",
    "print(loss_and_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
     ]
    }
   ],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(Dense(64, input_dim=20, activation='relu'))\n",
    "model1.add(Dropout(0.5))\n",
    "model1.add(Dense(64, activation='relu'))\n",
    "model1.add(Dropout(0.5))\n",
    "model1.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model1.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model1.fit(x_train, y_train,\n",
    "          epochs=20,\n",
    "          batch_size=128)\n",
    "score = model.evaluate(x_test, y_test, batch_size=128)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

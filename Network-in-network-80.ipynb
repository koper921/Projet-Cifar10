{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 23s    \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 192)       14592     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 192)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 160)       30880     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32, 32, 160)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 32, 96)        15456     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 32, 32, 96)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 96)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 192)       460992    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16, 16, 192)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 16, 16, 192)       37056     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 16, 16, 192)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 16, 16, 192)       37056     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16, 16, 192)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 192)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 8, 192)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 8, 8, 192)         331968    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 8, 8, 192)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 8, 8, 192)         37056     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 8, 8, 192)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 8, 8, 10)          1930      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 8, 8, 10)          0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 966,986\n",
      "Trainable params: 966,986\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Using real-time data augmentation.\n",
      "Epoch 1/80\n",
      "391/391 [==============================] - 59s - loss: 2.3692 - acc: 0.2099 - val_loss: 2.1462 - val_acc: 0.3292\n",
      "Epoch 2/80\n",
      "391/391 [==============================] - 60s - loss: 2.0627 - acc: 0.3718 - val_loss: 1.9630 - val_acc: 0.4273\n",
      "Epoch 3/80\n",
      "391/391 [==============================] - 59s - loss: 1.9136 - acc: 0.4369 - val_loss: 1.8022 - val_acc: 0.4562\n",
      "Epoch 4/80\n",
      "391/391 [==============================] - 59s - loss: 1.7283 - acc: 0.4761 - val_loss: 1.6801 - val_acc: 0.4980\n",
      "Epoch 5/80\n",
      "391/391 [==============================] - 59s - loss: 1.6319 - acc: 0.5076 - val_loss: 1.6222 - val_acc: 0.5266\n",
      "Epoch 6/80\n",
      "391/391 [==============================] - 59s - loss: 1.5129 - acc: 0.5520 - val_loss: 1.3350 - val_acc: 0.6019\n",
      "Epoch 7/80\n",
      "391/391 [==============================] - 59s - loss: 1.3076 - acc: 0.6089 - val_loss: 1.2604 - val_acc: 0.6233\n",
      "Epoch 8/80\n",
      "391/391 [==============================] - 59s - loss: 1.2282 - acc: 0.6397 - val_loss: 1.1276 - val_acc: 0.6707\n",
      "Epoch 9/80\n",
      "391/391 [==============================] - 59s - loss: 1.1634 - acc: 0.6634 - val_loss: 1.1101 - val_acc: 0.6825\n",
      "Epoch 10/80\n",
      "391/391 [==============================] - 59s - loss: 1.1139 - acc: 0.6813 - val_loss: 1.0388 - val_acc: 0.7113\n",
      "Epoch 11/80\n",
      "391/391 [==============================] - 59s - loss: 1.0724 - acc: 0.6964 - val_loss: 1.0070 - val_acc: 0.7205\n",
      "Epoch 12/80\n",
      "391/391 [==============================] - 59s - loss: 1.0330 - acc: 0.7113 - val_loss: 1.0162 - val_acc: 0.7142\n",
      "Epoch 13/80\n",
      "391/391 [==============================] - 59s - loss: 0.9984 - acc: 0.7212 - val_loss: 1.0225 - val_acc: 0.7167\n",
      "Epoch 14/80\n",
      "391/391 [==============================] - 59s - loss: 0.9701 - acc: 0.7327 - val_loss: 0.9941 - val_acc: 0.7340\n",
      "Epoch 15/80\n",
      "391/391 [==============================] - 59s - loss: 0.9417 - acc: 0.7448 - val_loss: 0.9299 - val_acc: 0.7483\n",
      "Epoch 16/80\n",
      "391/391 [==============================] - 59s - loss: 0.9126 - acc: 0.7557 - val_loss: 0.9443 - val_acc: 0.7475\n",
      "Epoch 17/80\n",
      "391/391 [==============================] - 59s - loss: 0.8910 - acc: 0.7599 - val_loss: 0.8889 - val_acc: 0.7666\n",
      "Epoch 18/80\n",
      "391/391 [==============================] - 59s - loss: 0.8720 - acc: 0.7697 - val_loss: 0.8567 - val_acc: 0.7769\n",
      "Epoch 19/80\n",
      "391/391 [==============================] - 59s - loss: 0.8503 - acc: 0.7733 - val_loss: 0.8921 - val_acc: 0.7634\n",
      "Epoch 20/80\n",
      "391/391 [==============================] - 59s - loss: 0.8367 - acc: 0.7793 - val_loss: 0.8060 - val_acc: 0.7892\n",
      "Epoch 21/80\n",
      "391/391 [==============================] - 59s - loss: 0.8181 - acc: 0.7856 - val_loss: 0.8319 - val_acc: 0.7832\n",
      "Epoch 22/80\n",
      "391/391 [==============================] - 59s - loss: 0.8037 - acc: 0.7915 - val_loss: 0.8100 - val_acc: 0.7915\n",
      "Epoch 23/80\n",
      "391/391 [==============================] - 59s - loss: 0.7933 - acc: 0.7953 - val_loss: 0.7636 - val_acc: 0.8053\n",
      "Epoch 24/80\n",
      "391/391 [==============================] - 59s - loss: 0.7791 - acc: 0.8007 - val_loss: 0.7833 - val_acc: 0.8064\n",
      "Epoch 25/80\n",
      "391/391 [==============================] - 59s - loss: 0.7630 - acc: 0.8082 - val_loss: 0.8109 - val_acc: 0.7892\n",
      "Epoch 26/80\n",
      "391/391 [==============================] - 59s - loss: 0.7522 - acc: 0.8099 - val_loss: 0.7514 - val_acc: 0.8094\n",
      "Epoch 27/80\n",
      "391/391 [==============================] - 59s - loss: 0.7425 - acc: 0.8122 - val_loss: 0.7292 - val_acc: 0.8223\n",
      "Epoch 28/80\n",
      "391/391 [==============================] - 59s - loss: 0.7327 - acc: 0.8170 - val_loss: 0.7591 - val_acc: 0.8139\n",
      "Epoch 29/80\n",
      "391/391 [==============================] - 59s - loss: 0.7225 - acc: 0.8193 - val_loss: 0.7293 - val_acc: 0.8220\n",
      "Epoch 30/80\n",
      "391/391 [==============================] - 59s - loss: 0.7174 - acc: 0.8208 - val_loss: 0.7240 - val_acc: 0.8244\n",
      "Epoch 31/80\n",
      "391/391 [==============================] - 59s - loss: 0.7010 - acc: 0.8262 - val_loss: 0.7590 - val_acc: 0.8099\n",
      "Epoch 32/80\n",
      "391/391 [==============================] - 59s - loss: 0.6972 - acc: 0.8285 - val_loss: 0.6977 - val_acc: 0.8288\n",
      "Epoch 33/80\n",
      "391/391 [==============================] - 59s - loss: 0.6830 - acc: 0.8321 - val_loss: 0.7423 - val_acc: 0.8217\n",
      "Epoch 34/80\n",
      "391/391 [==============================] - 59s - loss: 0.6743 - acc: 0.8367 - val_loss: 0.6654 - val_acc: 0.8409\n",
      "Epoch 35/80\n",
      "391/391 [==============================] - 59s - loss: 0.6697 - acc: 0.8353 - val_loss: 0.6607 - val_acc: 0.8400\n",
      "Epoch 36/80\n",
      "391/391 [==============================] - 59s - loss: 0.6609 - acc: 0.8400 - val_loss: 0.6868 - val_acc: 0.8339\n",
      "Epoch 37/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 59s - loss: 0.6585 - acc: 0.8401 - val_loss: 0.6791 - val_acc: 0.8403\n",
      "Epoch 38/80\n",
      "391/391 [==============================] - 59s - loss: 0.6525 - acc: 0.8423 - val_loss: 0.6803 - val_acc: 0.8337\n",
      "Epoch 39/80\n",
      "391/391 [==============================] - 59s - loss: 0.6430 - acc: 0.8450 - val_loss: 0.6765 - val_acc: 0.8390\n",
      "Epoch 40/80\n",
      "391/391 [==============================] - 59s - loss: 0.6341 - acc: 0.8470 - val_loss: 0.7002 - val_acc: 0.8321\n",
      "Epoch 41/80\n",
      "391/391 [==============================] - 59s - loss: 0.6331 - acc: 0.8486 - val_loss: 0.6997 - val_acc: 0.8306\n",
      "Epoch 42/80\n",
      "391/391 [==============================] - 59s - loss: 0.6246 - acc: 0.8518 - val_loss: 0.6547 - val_acc: 0.8430\n",
      "Epoch 43/80\n",
      "391/391 [==============================] - 59s - loss: 0.6206 - acc: 0.8526 - val_loss: 0.6528 - val_acc: 0.8499\n",
      "Epoch 44/80\n",
      "391/391 [==============================] - 59s - loss: 0.6142 - acc: 0.8531 - val_loss: 0.6658 - val_acc: 0.8464\n",
      "Epoch 45/80\n",
      "391/391 [==============================] - 59s - loss: 0.6083 - acc: 0.8580 - val_loss: 0.6521 - val_acc: 0.8451\n",
      "Epoch 46/80\n",
      "391/391 [==============================] - 59s - loss: 0.6023 - acc: 0.8592 - val_loss: 0.7119 - val_acc: 0.8297\n",
      "Epoch 47/80\n",
      "391/391 [==============================] - 59s - loss: 0.5973 - acc: 0.8602 - val_loss: 0.6485 - val_acc: 0.8486\n",
      "Epoch 48/80\n",
      "391/391 [==============================] - 59s - loss: 0.5927 - acc: 0.8605 - val_loss: 0.6653 - val_acc: 0.8475\n",
      "Epoch 49/80\n",
      "391/391 [==============================] - 59s - loss: 0.5846 - acc: 0.8641 - val_loss: 0.6462 - val_acc: 0.8518\n",
      "Epoch 50/80\n",
      "391/391 [==============================] - 59s - loss: 0.5819 - acc: 0.8647 - val_loss: 0.6471 - val_acc: 0.8509\n",
      "Epoch 51/80\n",
      "391/391 [==============================] - 59s - loss: 0.5766 - acc: 0.8676 - val_loss: 0.6536 - val_acc: 0.8473\n",
      "Epoch 52/80\n",
      "391/391 [==============================] - 59s - loss: 0.5732 - acc: 0.8682 - val_loss: 0.6527 - val_acc: 0.8493\n",
      "Epoch 53/80\n",
      "391/391 [==============================] - 59s - loss: 0.5709 - acc: 0.8681 - val_loss: 0.6424 - val_acc: 0.8548\n",
      "Epoch 54/80\n",
      "391/391 [==============================] - 59s - loss: 0.5663 - acc: 0.8708 - val_loss: 0.6443 - val_acc: 0.8520\n",
      "Epoch 55/80\n",
      "391/391 [==============================] - 59s - loss: 0.5632 - acc: 0.8719 - val_loss: 0.6127 - val_acc: 0.8658\n",
      "Epoch 56/80\n",
      "391/391 [==============================] - 59s - loss: 0.5574 - acc: 0.8718 - val_loss: 0.5866 - val_acc: 0.8658\n",
      "Epoch 57/80\n",
      "391/391 [==============================] - 59s - loss: 0.5502 - acc: 0.8743 - val_loss: 0.6316 - val_acc: 0.8563\n",
      "Epoch 58/80\n",
      "391/391 [==============================] - 59s - loss: 0.5506 - acc: 0.8755 - val_loss: 0.6409 - val_acc: 0.8518\n",
      "Epoch 59/80\n",
      "391/391 [==============================] - 59s - loss: 0.5510 - acc: 0.8748 - val_loss: 0.6126 - val_acc: 0.8621\n",
      "Epoch 60/80\n",
      "391/391 [==============================] - 59s - loss: 0.5416 - acc: 0.8782 - val_loss: 0.6099 - val_acc: 0.8613\n",
      "Epoch 61/80\n",
      "391/391 [==============================] - 59s - loss: 0.5398 - acc: 0.8770 - val_loss: 0.6382 - val_acc: 0.8577\n",
      "Epoch 62/80\n",
      "391/391 [==============================] - 59s - loss: 0.5354 - acc: 0.8799 - val_loss: 0.6290 - val_acc: 0.8592\n",
      "Epoch 63/80\n",
      "391/391 [==============================] - 59s - loss: 0.5342 - acc: 0.8818 - val_loss: 0.5896 - val_acc: 0.8661\n",
      "Epoch 64/80\n",
      "391/391 [==============================] - 59s - loss: 0.5254 - acc: 0.8840 - val_loss: 0.6263 - val_acc: 0.8605\n",
      "Epoch 65/80\n",
      "391/391 [==============================] - 59s - loss: 0.5234 - acc: 0.8835 - val_loss: 0.6400 - val_acc: 0.8558\n",
      "Epoch 66/80\n",
      "391/391 [==============================] - 59s - loss: 0.5209 - acc: 0.8852 - val_loss: 0.6071 - val_acc: 0.8645\n",
      "Epoch 67/80\n",
      "391/391 [==============================] - 59s - loss: 0.5242 - acc: 0.8847 - val_loss: 0.6170 - val_acc: 0.8601\n",
      "Epoch 68/80\n",
      "391/391 [==============================] - 59s - loss: 0.5141 - acc: 0.8868 - val_loss: 0.6413 - val_acc: 0.8553\n",
      "Epoch 69/80\n",
      "391/391 [==============================] - 59s - loss: 0.5141 - acc: 0.8872 - val_loss: 0.6302 - val_acc: 0.8560\n",
      "Epoch 70/80\n",
      "391/391 [==============================] - 59s - loss: 0.5146 - acc: 0.8861 - val_loss: 0.6094 - val_acc: 0.8656\n",
      "Epoch 71/80\n",
      "391/391 [==============================] - 59s - loss: 0.5099 - acc: 0.8876 - val_loss: 0.6239 - val_acc: 0.8628\n",
      "Epoch 72/80\n",
      "391/391 [==============================] - 59s - loss: 0.5055 - acc: 0.8888 - val_loss: 0.5704 - val_acc: 0.8717\n",
      "Epoch 73/80\n",
      "391/391 [==============================] - 59s - loss: 0.5081 - acc: 0.8894 - val_loss: 0.6021 - val_acc: 0.8601\n",
      "Epoch 74/80\n",
      "391/391 [==============================] - 59s - loss: 0.5012 - acc: 0.8908 - val_loss: 0.6050 - val_acc: 0.8679\n",
      "Epoch 75/80\n",
      "391/391 [==============================] - 59s - loss: 0.4985 - acc: 0.8915 - val_loss: 0.5868 - val_acc: 0.8687\n",
      "Epoch 76/80\n",
      "391/391 [==============================] - 59s - loss: 0.4992 - acc: 0.8935 - val_loss: 0.6157 - val_acc: 0.8670\n",
      "Epoch 77/80\n",
      "391/391 [==============================] - 59s - loss: 0.4936 - acc: 0.8940 - val_loss: 0.5936 - val_acc: 0.8699\n",
      "Epoch 78/80\n",
      "391/391 [==============================] - 59s - loss: 0.4947 - acc: 0.8933 - val_loss: 0.6098 - val_acc: 0.8620\n",
      "Epoch 79/80\n",
      "391/391 [==============================] - 59s - loss: 0.4856 - acc: 0.8960 - val_loss: 0.5644 - val_acc: 0.8738\n",
      "Epoch 80/80\n",
      "391/391 [==============================] - 59s - loss: 0.4891 - acc: 0.8939 - val_loss: 0.6044 - val_acc: 0.8663\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, AveragePooling2D\n",
    "from keras.initializers import RandomNormal  \n",
    "from keras import optimizers\n",
    "from keras.callbacks import LearningRateScheduler, TensorBoard\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "batch_size    = 128\n",
    "epochs        = 80\n",
    "iterations    = 391\n",
    "num_classes   = 10\n",
    "dropout       = 0.5\n",
    "weight_decay  = 0.0001\n",
    "log_filepath  = './nin'\n",
    "\n",
    "def color_preprocessing(x_train,x_test):\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    mean = [125.307, 122.95, 113.865]\n",
    "    std  = [62.9932, 62.0887, 66.7048]\n",
    "    for i in range(3):\n",
    "        x_train[:,:,:,i] = (x_train[:,:,:,i] - mean[i]) / std[i]\n",
    "        x_test[:,:,:,i] = (x_test[:,:,:,i] - mean[i]) / std[i]\n",
    "\n",
    "    return x_train, x_test\n",
    "\n",
    "def scheduler(epoch):\n",
    "    if epoch <= 80:\n",
    "        return 0.01\n",
    "    if epoch <= 140:\n",
    "        return 0.005\n",
    "    return 0.001\n",
    "\n",
    "def build_model():\n",
    "  model = Sequential()\n",
    "\n",
    "  model.add(Conv2D(192, (5, 5), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=\"he_normal\", input_shape=x_train.shape[1:]))\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(Conv2D(160, (1, 1), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=\"he_normal\"))\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(Conv2D(96, (1, 1), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=\"he_normal\"))\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(MaxPooling2D(pool_size=(3, 3),strides=(2,2),padding = 'same'))\n",
    "  \n",
    "  model.add(Dropout(dropout))\n",
    "  \n",
    "  model.add(Conv2D(192, (5, 5), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=\"he_normal\"))\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(Conv2D(192, (1, 1),padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=\"he_normal\"))\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(Conv2D(192, (1, 1),padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=\"he_normal\"))\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(MaxPooling2D(pool_size=(3, 3),strides=(2,2),padding = 'same'))\n",
    "  \n",
    "  model.add(Dropout(dropout))\n",
    "  \n",
    "  model.add(Conv2D(192, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=\"he_normal\"))\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(Conv2D(192, (1, 1), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=\"he_normal\"))\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(Conv2D(10, (1, 1), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=\"he_normal\"))\n",
    "  model.add(Activation('relu'))\n",
    "  \n",
    "  model.add(GlobalAveragePooling2D())\n",
    "  model.add(Activation('softmax'))\n",
    "  \n",
    "  sgd = optimizers.SGD(lr=.1, momentum=0.9, nesterov=True)\n",
    "  model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # load data\n",
    "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "    \n",
    "    x_train, x_test = color_preprocessing(x_train, x_test)\n",
    "\n",
    "    # build network\n",
    "    model = build_model()\n",
    "    print(model.summary())\n",
    "\n",
    "    # set callback\n",
    "    tb_cb = TensorBoard(log_dir=log_filepath, histogram_freq=0)\n",
    "    change_lr = LearningRateScheduler(scheduler)\n",
    "    cbks = [change_lr,tb_cb]\n",
    "\n",
    "    # set data augmentation\n",
    "    print('Using real-time data augmentation.')\n",
    "    datagen = ImageDataGenerator(horizontal_flip=True,width_shift_range=0.125,height_shift_range=0.125,fill_mode='constant',cval=0.)\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    # start training\n",
    "    model.fit_generator(datagen.flow(x_train, y_train,batch_size=batch_size),steps_per_epoch=iterations,epochs=epochs,callbacks=cbks,validation_data=(x_test, y_test))\n",
    "    model.save('nin.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
